{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Medical_Chatbot",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3892155,
          "sourceType": "datasetVersion",
          "datasetId": 2308952
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chipojaya1/medical_chatbot/blob/main/Medical_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "karthikudyawar_disease_symptom_prediction_path = kagglehub.dataset_download('karthikudyawar/disease-symptom-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "6b0dwBgRlBOX"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSDC Data Science Projects"
      ],
      "metadata": {
        "id": "DTfH0tyAQ-8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developing a Medical Chatbot Using RAG and LLMs\n",
        "\n",
        "## Name: MedBot4U"
      ],
      "metadata": {
        "id": "SjM8rSwyQ-8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "dyyca9d-Q-8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Instructions\n",
        "\n",
        "**Please follow the instructions below before proceeding with the project!**  \n",
        "\n",
        "In this project, we will utilize the GPU provided by Kaggle. The GPU will be used to train and infer LLMs.  \n",
        "To activate the GPU, follow the steps outlined in this [document](https://drive.google.com/file/d/10KHE4eJJKkF9TLxEpSBVKFFqAP41yVh0/view?usp=sharing).  \n",
        "\n",
        "❗️It's important to note that the GPU quota is 30 hours per week. While this is a sizable allocation, it's always a good practice to monitor your usage to ensure you stay within the limit."
      ],
      "metadata": {
        "id": "6fvmzlQxQ-8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "9Vqpu_mOQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **❗️Disclaimer❗️**\n",
        "The chatbot developed in this project is **not a substitute for professional medical diagnosis**. Its responses are generated based solely on the dataset it was trained on, which is limited in scope and not clinically comprehensive. Please do not rely on its outputs for making medical decisions.\n",
        "\n",
        "Always consult a licensed healthcare provider for any health concerns.\n",
        "\n",
        "If you are in an emergency situation, please seek immediate medical attention. You can find a list of emergency contact numbers worldwide [in this link.](https://www.dt.com/ca/wp-content/uploads/2017/03/Global-_911_Emergency-Contacts.pdf)"
      ],
      "metadata": {
        "id": "uU1GgRQbQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "40ot6I86Q-8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Introduction\n",
        "\n",
        "**Motivation:** When people feel unwell, they often search online to understand their symptoms but the information they find can be confusing. Visiting a doctor isn’t always immediately possible, especially in remote areas. A medical chatbot can help bridge this gap by giving quick, easy-to-understand information about possible health conditions. It can guide users to make better decisions about whether to seek medical help, all from the comfort of their home.\n",
        "\n",
        "**What are LLMs?**\n",
        "A Large Language Model (LLM) is an advanced AI model that can understand and generate human-like text. It is trained on a large amount of text data and can answer questions, write content, summarize information and hold conversations.\n",
        "\n",
        "**Why LLMs?**\n",
        "LLMs can understand and generate natural human language, making them ideal for building chatbots. They are capable of handling complex queries, providing detailed responses and adapting to different ways people describe their symptoms."
      ],
      "metadata": {
        "id": "_vRsdJKJQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Outline\n",
        "\n",
        "1. Data Preprocessing\n",
        "2. Implementing a simple **rule-based** chatbot\n",
        "3. Developing a chatbot using **text embeddings** and **RAG**\n",
        "4. **Fine-tuning** LLMs for our specific data and use-case"
      ],
      "metadata": {
        "id": "pTeVP-hWQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "nUcYTQnZQ-8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "7h2FXEcgQ-8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import all the necessary libraries which will be used for data preprocessing as well as moving forward in building the chatbot.\n",
        "\n",
        "The **warnings** library is used to supress unimportant warnings while we run the cells"
      ],
      "metadata": {
        "id": "RRDbRc69Q-8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pupy6zmRQ-8Z",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.724918Z",
          "iopub.execute_input": "2025-10-06T21:54:47.725246Z",
          "iopub.status.idle": "2025-10-06T21:54:47.729292Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.725193Z",
          "shell.execute_reply": "2025-10-06T21:54:47.728642Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Reading the data\n",
        "\n",
        "We will import a dataset from Kaggle. This step does not require us to download the dataset and we can directly access it using the /kaggle/input path. For this we will first have to add the dataset to our Kaggle notebook.  \n",
        "\n",
        "**To add the data follow the steps below**:\n",
        "1. Click *Input* on the right menu bar\n",
        "2. Select *+ Add Input*\n",
        "3. Enter this URL in the search bar: https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data\n",
        "4. Click on the *+* icon to add the dataset to the notebook\n",
        "\n",
        "You can explore the data we are using for this project [here](https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data)\n",
        "\n",
        "In this project, we will only use **dataset.csv** which contains the disease and its corresponding symptoms list"
      ],
      "metadata": {
        "id": "S4iyPYVJQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "dis_symp_df = pd.read_csv(\"/kaggle/input/disease-symptom-prediction/dataset.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "16yf0f9rQ-8b",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.730399Z",
          "iopub.execute_input": "2025-10-06T21:54:47.730885Z",
          "iopub.status.idle": "2025-10-06T21:54:47.771232Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.730851Z",
          "shell.execute_reply": "2025-10-06T21:54:47.770663Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how the dataset is structured using the pandas `head` function"
      ],
      "metadata": {
        "id": "Zg6sLyfmQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "iMiBd98PawCJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.771938Z",
          "iopub.execute_input": "2025-10-06T21:54:47.772114Z",
          "iopub.status.idle": "2025-10-06T21:54:47.79259Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.772099Z",
          "shell.execute_reply": "2025-10-06T21:54:47.79191Z"
        },
        "outputId": "d8f1323a-41f2-454e-9f2f-14f7e0db7d1b"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease           Symptom_0           Symptom_1                Symptom_2  \\\n0    AIDS      muscle_wasting   patches_in_throat               high_fever   \n1    AIDS   patches_in_throat          high_fever   extra_marital_contacts   \n2    AIDS      muscle_wasting          high_fever   extra_marital_contacts   \n3    AIDS      muscle_wasting   patches_in_throat   extra_marital_contacts   \n4    AIDS      muscle_wasting   patches_in_throat               high_fever   \n\n                 Symptom_3 Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8  \\\n0   extra_marital_contacts       NaN       NaN       NaN       NaN       NaN   \n1                      NaN       NaN       NaN       NaN       NaN       NaN   \n2                      NaN       NaN       NaN       NaN       NaN       NaN   \n3                      NaN       NaN       NaN       NaN       NaN       NaN   \n4                      NaN       NaN       NaN       NaN       NaN       NaN   \n\n  Symptom_9 Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n0       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n1       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n2       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n3       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n4       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n\n  Symptom_16  \n0        NaN  \n1        NaN  \n2        NaN  \n3        NaN  \n4        NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptom_0</th>\n      <th>Symptom_1</th>\n      <th>Symptom_2</th>\n      <th>Symptom_3</th>\n      <th>Symptom_4</th>\n      <th>Symptom_5</th>\n      <th>Symptom_6</th>\n      <th>Symptom_7</th>\n      <th>Symptom_8</th>\n      <th>Symptom_9</th>\n      <th>Symptom_10</th>\n      <th>Symptom_11</th>\n      <th>Symptom_12</th>\n      <th>Symptom_13</th>\n      <th>Symptom_14</th>\n      <th>Symptom_15</th>\n      <th>Symptom_16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the symptoms have underscores which need to be removed\n",
        "\n",
        "#### Step 2: Remove Underscores from the Symptoms Text\n",
        "\n",
        "Steps to remove the underscores:\n",
        "1. Find all columns in dis_symp_df where the column names start with \"Symptom\"\n",
        "2. After finding those columns, replace the _ with a blank space"
      ],
      "metadata": {
        "id": "_J7Hcn5FkhPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove underscores from symptoms text\n",
        "symptom_cols = [col for col in dis_symp_df.columns if col.startswith(\"Symptom_\")]\n",
        "dis_symp_df[symptom_cols] = dis_symp_df[symptom_cols].replace(\"_\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "7mLnNQGkcX6P",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.794146Z",
          "iopub.execute_input": "2025-10-06T21:54:47.794377Z",
          "iopub.status.idle": "2025-10-06T21:54:47.805895Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.794361Z",
          "shell.execute_reply": "2025-10-06T21:54:47.805326Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify if the underscores have been removed by printing the first few entries of the dataframe"
      ],
      "metadata": {
        "id": "EsRk8LsOQ-8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the changes\n",
        "print(\"After removing underscores (first 5 rows):\")\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "VQEE0EAPkX-a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.806525Z",
          "iopub.execute_input": "2025-10-06T21:54:47.806687Z",
          "iopub.status.idle": "2025-10-06T21:54:47.829098Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.806675Z",
          "shell.execute_reply": "2025-10-06T21:54:47.828536Z"
        },
        "outputId": "2292527b-6693-4035-c990-645f097030a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "After removing underscores (first 5 rows):\n",
          "output_type": "stream"
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease           Symptom_0           Symptom_1                Symptom_2  \\\n0    AIDS      muscle wasting   patches in throat               high fever   \n1    AIDS   patches in throat          high fever   extra marital contacts   \n2    AIDS      muscle wasting          high fever   extra marital contacts   \n3    AIDS      muscle wasting   patches in throat   extra marital contacts   \n4    AIDS      muscle wasting   patches in throat               high fever   \n\n                 Symptom_3 Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8  \\\n0   extra marital contacts       NaN       NaN       NaN       NaN       NaN   \n1                      NaN       NaN       NaN       NaN       NaN       NaN   \n2                      NaN       NaN       NaN       NaN       NaN       NaN   \n3                      NaN       NaN       NaN       NaN       NaN       NaN   \n4                      NaN       NaN       NaN       NaN       NaN       NaN   \n\n  Symptom_9 Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n0       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n1       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n2       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n3       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n4       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n\n  Symptom_16  \n0        NaN  \n1        NaN  \n2        NaN  \n3        NaN  \n4        NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptom_0</th>\n      <th>Symptom_1</th>\n      <th>Symptom_2</th>\n      <th>Symptom_3</th>\n      <th>Symptom_4</th>\n      <th>Symptom_5</th>\n      <th>Symptom_6</th>\n      <th>Symptom_7</th>\n      <th>Symptom_8</th>\n      <th>Symptom_9</th>\n      <th>Symptom_10</th>\n      <th>Symptom_11</th>\n      <th>Symptom_12</th>\n      <th>Symptom_13</th>\n      <th>Symptom_14</th>\n      <th>Symptom_15</th>\n      <th>Symptom_16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transforming the Structure of the Dataset\n",
        "\n",
        "We need to transform this dataset to match input-output pairs that are suitable for training LLMs\n",
        "\n",
        "Two Methods:\n",
        "\n",
        "\n",
        "1.   Pivot Longer: will result in a bigger dataset and more suited for simple classification tasks\n",
        "2.   Comma-separated symptoms in one column: suitable for sentence-level input from users\n",
        "\n",
        "Therefore, we will proceed with combining the symptoms into one column and separating them with commas\n",
        "\n"
      ],
      "metadata": {
        "id": "734MMipLcMHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to create a comma-separated list of symptoms:\n",
        "\n",
        "\n",
        "1. Combine the symptom values row-wise  \n",
        "For each row, go through the values in the symptom_cols:\n",
        "* Skip any missing values\n",
        "* Join the non-missing symptom strings with commas\n",
        "* Store this in a new column called **Symptoms**\n",
        "\n",
        "2. Remove all the original symptoms columns"
      ],
      "metadata": {
        "id": "OKpEsnqDQ-8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a lambda function to combine the symptom values row-wise  \n",
        "\n",
        "**What is a lambda function?**  \n",
        "`square = lambda x: x*x`  \n",
        "`print(square(5))`\n",
        "\n",
        "`lambda x: x*x` is a lambda function that takes x as input and returns x*x in just one, short line"
      ],
      "metadata": {
        "id": "ORUzm71aQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the symptom values row-wise\n",
        "dis_symp_df[\"Symptoms\"] = dis_symp_df[symptom_cols].apply(lambda row: ', '.join([s for s in row if pd.notna(s)]), axis=1)\n",
        "dis_symp_df.drop(symptom_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "52j6BfrDlMFA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.829769Z",
          "iopub.execute_input": "2025-10-06T21:54:47.830005Z",
          "iopub.status.idle": "2025-10-06T21:54:47.848613Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.829986Z",
          "shell.execute_reply": "2025-10-06T21:54:47.847929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the updated dataframe again using the `head` function"
      ],
      "metadata": {
        "id": "Jk-QM5BLQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame after combining symptoms:\")\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "s9NwCLB9mq20",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.849425Z",
          "iopub.execute_input": "2025-10-06T21:54:47.849687Z",
          "iopub.status.idle": "2025-10-06T21:54:47.868028Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.849665Z",
          "shell.execute_reply": "2025-10-06T21:54:47.867258Z"
        },
        "outputId": "4ecbe300-45f9-43c9-b1b4-627c0ab1c422"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DataFrame after combining symptoms:\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease                                           Symptoms\n0    AIDS   muscle wasting,  patches in throat,  high fev...\n1    AIDS   patches in throat,  high fever,  extra marita...\n2    AIDS   muscle wasting,  high fever,  extra marital c...\n3    AIDS   muscle wasting,  patches in throat,  extra ma...\n4    AIDS    muscle wasting,  patches in throat,  high fever",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  high fev...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches in throat,  high fever,  extra marita...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  high fever,  extra marital c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  extra ma...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  high fever</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Check for Duplicate Lists\n",
        "\n",
        "Obtain a summary of the dataset using `info` function"
      ],
      "metadata": {
        "id": "5dstfR8wQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset info before removing duplicates:\")\n",
        "print(dis_symp_df.info())\n",
        "print(f\"\\nNumber of rows before removing duplicates: {len(dis_symp_df)}\")"
      ],
      "metadata": {
        "id": "H7AeYZIgnmnv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.86892Z",
          "iopub.execute_input": "2025-10-06T21:54:47.869163Z",
          "iopub.status.idle": "2025-10-06T21:54:47.889593Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.869141Z",
          "shell.execute_reply": "2025-10-06T21:54:47.888936Z"
        },
        "outputId": "98545326-d52a-4429-df46-65cb9f0f4827"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset info before removing duplicates:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 313 entries, 0 to 312\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Disease   313 non-null    object\n 1   Symptoms  313 non-null    object\ndtypes: object(2)\nmemory usage: 5.0+ KB\nNone\n\nNumber of rows before removing duplicates: 313\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicate columns if they have the same symptoms list  \n",
        "**Hint**: Use the `drop_duplicates` function only for the **Symptoms** column"
      ],
      "metadata": {
        "id": "cwwWxjf-pIGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate columns with the same symptoms list\n",
        "initial_count = len(dis_symp_df)\n",
        "dis_symp_df.drop_duplicates(subset=['Symptoms'], inplace=True)\n",
        "final_count = len(dis_symp_df)"
      ],
      "metadata": {
        "id": "2S3JYj1gnoHX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.891267Z",
          "iopub.execute_input": "2025-10-06T21:54:47.891443Z",
          "iopub.status.idle": "2025-10-06T21:54:47.905216Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.89143Z",
          "shell.execute_reply": "2025-10-06T21:54:47.904517Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the summary of the dataset again to see if there were any duplicates. Report back on your conclusion."
      ],
      "metadata": {
        "id": "kFegXNooQ-8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nNumber of rows after removing duplicates: {len(dis_symp_df)}\")\n",
        "print(f\"Number of duplicates removed: {initial_count - final_count}\")"
      ],
      "metadata": {
        "id": "HlQ7KxTgo8I6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.90582Z",
          "iopub.execute_input": "2025-10-06T21:54:47.905976Z",
          "iopub.status.idle": "2025-10-06T21:54:47.920721Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.905963Z",
          "shell.execute_reply": "2025-10-06T21:54:47.919979Z"
        },
        "outputId": "d1d49ba6-6412-4ce5-8249-a919e2a15752"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nNumber of rows after removing duplicates: 313\nNumber of duplicates removed: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final dataframe\n",
        "print(\"\\nFinal DataFrame:\")\n",
        "print(dis_symp_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:47.921361Z",
          "iopub.execute_input": "2025-10-06T21:54:47.92154Z",
          "iopub.status.idle": "2025-10-06T21:54:47.935749Z",
          "shell.execute_reply.started": "2025-10-06T21:54:47.921526Z",
          "shell.execute_reply": "2025-10-06T21:54:47.935247Z"
        },
        "id": "WbPbcbyHlBOf",
        "outputId": "2d70bd60-cd8a-4608-ed75-49f6ca5b6c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nFinal DataFrame:\n  Disease                                           Symptoms\n0    AIDS   muscle wasting,  patches in throat,  high fev...\n1    AIDS   patches in throat,  high fever,  extra marita...\n2    AIDS   muscle wasting,  high fever,  extra marital c...\n3    AIDS   muscle wasting,  patches in throat,  extra ma...\n4    AIDS    muscle wasting,  patches in throat,  high fever\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Your conclusion here***"
      ],
      "metadata": {
        "id": "qOo-3JZxpPGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "NUNwR6pZQ-8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 2: Rule-Based Chatbot (Cosine Similarity)"
      ],
      "metadata": {
        "id": "SeBqC4pl4Ma1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now be implementing one of the most basic versions of a chatbot: **a rule-based chatbot using cosine similarity**.\n",
        "\n",
        "**Cosine similarity** is a metric used to measure how similar two vectors are, regardless of their magnitude.\n",
        "\n",
        "A **rule-based chatbot** using cosine similarity identifies the most appropriate response by comparing the user’s input with a set of predefined statements and selecting the one with the highest semantic similarity based on cosine similarity of their vector embeddings."
      ],
      "metadata": {
        "id": "zKtXgjG7Q-8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Import required packages from `sklearn`\n",
        "\n",
        "`TfidfVectorizer`: Read up more on Term Frequency-Inverse Document Frequency (TF-IDF) [here](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/). This is used to convert text into numerical vectors based on how important each word is.\n",
        "\n",
        "`cosine_similarity`: Package used to measure how similar the user's symptom input is to each disease's symptom list."
      ],
      "metadata": {
        "id": "B5LUmH6zQ-8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the two key tools for comparing text documents\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   # for converting a list of text documents into numerical vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity        # for measuring how similar two sources are based on the words they contain"
      ],
      "metadata": {
        "id": "GHwuLmIK4P7x",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Group All Symptom Entries For Each Disease into a Single String"
      ],
      "metadata": {
        "id": "ytLWF2-AQ-8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** Use a lambda function again. Group by **Disease** and then apply the lambda function to **Symptoms**"
      ],
      "metadata": {
        "id": "QTPfd5nnQ-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group all symptoms for each disease\n",
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(x)).reset_index()"
      ],
      "metadata": {
        "id": "KqjPU44A4VI2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Vectorize only the **Symptoms** column from dis_symp_df using `TfidfVectorizer` and `fit_transform()`"
      ],
      "metadata": {
        "id": "YXkxDB64Q-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize only the Symptoms\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(dis_symp_df[\"Symptoms\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "x0Or1EPCQ-8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "j3KfZi1GlBOn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "wZlHWTKQQ-8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    # Welcome message for the user\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Continue asking the user for input until they enter 'exit' or 'quit'\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # Converts the user's input into a TF-IDF vector using the previously trained vectorizer\n",
        "        user_vec = vectorizer.transform([user_input])\n",
        "\n",
        "        # Compares the user's vector with all disease-symptom vectors in the tfidf_matrix using cosine similarity\n",
        "        # flatten() is used to convert the 2D result into a 1D array of vectors\n",
        "        cosine_sim = cosine_similarity(user_vec, tfidf_matrix).flatten()\n",
        "\n",
        "\n",
        "        # Sorts the similarity scores in descending order and retrieves the top 3 indices\n",
        "        top_indices = cosine_sim.argsort()[::-1][:3]\n",
        "\n",
        "        # Creates a list of (disease name, similarity score) tuples and only includes matches where the score is >0.5\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            if cosine_sim[i] > 0.2:\n",
        "                disease = dis_symp_df.iloc[i][\"Disease\"]\n",
        "                score = cosine_sim[i]\n",
        "                results.append(disease)\n",
        "\n",
        "        if not results:\n",
        "            print(\"ChatBot: I couldn not find a good match for your symptoms. Try rephrasing or listing more symptoms.\\n\")\n",
        "            continue\n",
        "\n",
        "        # If there are results, print the top-matching diseases with their similarity scores\n",
        "        print(\"ChatBot: Based on your symptoms, here are possible conditions:\")\n",
        "        for i, (disease) in enumerate(results, 1):\n",
        "            print(f\"   {i}. {disease}\")\n",
        "\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "MfXnIvff4jt6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "tziZARey7iW4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of Rule-Based Technique\n",
        "\n",
        "\n",
        "*   Does not generalize well to unseen data since there is no training involved\n",
        "*   Not scalable\n",
        "\n"
      ],
      "metadata": {
        "id": "uco9GYd-AIiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "qqj2hXb-Q-8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 3: Embeddings + RAG"
      ],
      "metadata": {
        "id": "uwgAP0vpAUoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Retrieval Augmented Generation (RAG)?**\n",
        "* RAG allows a model to use external data it hasn’t been explicitly trained on\n",
        "* It addresses common LLM limitations like lack of real-time information and outdated knowledge\n",
        "* It works by converting both user queries and a knowledge base into vector embeddings\n",
        "* Uses similarity search to retrieve the most relevant context from the knowledge base\n",
        "* This retrieved context is appended to the user query and passed to the LLM to generate a more accurate and informed response"
      ],
      "metadata": {
        "id": "zuaYgbbUQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings** are numerical representations of text that capture its meaning and semantic similarity in a vector space."
      ],
      "metadata": {
        "id": "QVASENgRQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over the Rule-Based Method\n",
        "* Flexible and Scalable: Unlike TF-IDF which depends on exact word matches, embedding-based retrieval finds relevant records based on context and similarity in meaning\n",
        "* More Robust: Since embeddings capture the semantic meaning behind words and generalize over language structure, minor spelling errors or synonymns do not affect performance, unlike TF-IDF which is sensitive to exact tokens\n",
        "* Context-Aware Responses: RAG combines retrieval with an LLM allowing it to generate human-like responses instead of returning pre-written text\n",
        "* Easier to Update Knowledge: New information can be added to the embedding database without retraining the LLM"
      ],
      "metadata": {
        "id": "VonKsE9WQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the project, we will be implementing a RAG-based chatbot using SentenceTransformer to create embeddings and the Llama-2 LLM to generate responses"
      ],
      "metadata": {
        "id": "KoC4MkBQQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** After running some of these cells, you may get warnings in a red box. Warnings are messages that alert us about possible issues in the code that aren't severe enough to stop execution. This is totally normal and you can still proceed with implementing the chatbot!"
      ],
      "metadata": {
        "id": "py1EuqT7Q-8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install the auto-gptq and optimum libraries\n",
        "\n",
        "**auto-gptq:** used for loading and running quantized versions of large language models efficiently (more on this in Milestone 4)  \n",
        "**optimum:** a library by Hugging Face that helps optimize model inference and training, particularly with quantized models"
      ],
      "metadata": {
        "id": "Lb8IasSXQ-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq\n",
        "!pip install optimum"
      ],
      "metadata": {
        "id": "E1SWpwPmTH1v",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**torch:** imports PyTorch, a popular deep learning framework used for model loading, tensor computations and training/inference  \n",
        "**transformers:** Hugging Face's library for working with pretrained models  \n",
        "**AutoTokenizer:** automatically loads the appropriate tokenizer for a given model  \n",
        "**AutoModelForCausalLM:** loads a causal language model (used for text generation)  \n",
        "**sentence_transformers:** a library for generating embeddings (vector representations) of sentences  \n",
        "**SentenceTransformer:** Loads a model to convert text into embeddings  \n",
        "**util:** Provides utility functions like `semantic_search()` for comparing embeddings."
      ],
      "metadata": {
        "id": "bEOd5Iz0Q-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "phb243S_AatK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Prepare the dataset for embeddings\n",
        "\n",
        "1. Group rows by disease, join all symptoms into one sentence and convert grouped result back into a dataframe using `reset_index()`\n",
        "2. Create a **Text** column with the combined disease and its respective symptom list\n",
        "3. Convert the **Text** column into a list called **corpus** which will be used for generating embeddings. The corpus is our knowledge base."
      ],
      "metadata": {
        "id": "s1_reunRQ-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(x)).reset_index()\n",
        "dis_symp_df[\"Text\"] = dis_symp_df.apply(lambda row: f\"Disease: {row['Disease']}. Symptoms: {row['Symptoms']}\", axis=1)\n",
        "corpus = dis_symp_df[\"Text\"].tolist()"
      ],
      "metadata": {
        "id": "jDOl20HOK1el",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Transform the corpus into vector embeddings\n",
        "\n",
        "1. Load the pre-trained embeddings model `all-MiniLM-L6-v2` from SentenceTransformer\n",
        "2. Convert each text entry in the corpus into its numerical representation using `encode` and set `convert_to_tensor` to **True** to ensure that the output is in the PyTorch tensor format"
      ],
      "metadata": {
        "id": "_UXcnOJ6Q-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = embed_model.encode(corpus, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "EUiEGwdzLYGt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Load the LLM and its tokenizer\n",
        "\n",
        "1. Specify the pre-trained model. Here, we will be using a quantized (compressed) version of Llama-2-7B-Chat model fine-tuned and optimized by the TheBloke using GPTQ. You can read up more about it in this [link](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ).\n",
        "2. Load the corresponding tokenizer using `AutoTokenizer`\n",
        "3. Load the Llama-2 model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "ZDh5wao4Q-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Llama-2?**  \n",
        "LLaMA-2 is a family of open-source LLMs developed by Meta designed for natural language understanding and generation tasks"
      ],
      "metadata": {
        "id": "IYFbPuOgQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "OkE9uVS7LjsF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Generate a response from the Llama-2 model\n",
        "\n",
        "1. Tokenize the prompt\n",
        "2. Generate the output using sampling paramaters such as `max_new_tokens`, `do_sample`, `temperature` and `top_p`\n",
        "3. Decode the response into a readable string using `decode`\n",
        "4. Remove the original prompt text and return only the generated response"
      ],
      "metadata": {
        "id": "F77YjmJwQ-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Parameters:**  \n",
        "`max_new_tokens`: controls the length of the generated response  \n",
        "`do_sample`: enables sampling, picks the next token randomly based on the predicted probability distribution  \n",
        "`temperature`: a lower value gives a more factual response while a higher value could lead to potential hallucination  \n",
        "`top_p`: a higher value ensures the model avoids rare and low probability words"
      ],
      "metadata": {
        "id": "XZMc28QAQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama2_response(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3nGdPLuIQ-8s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Generate a response based on our dataset using RAG\n",
        "\n",
        "1. Convert the user query into embeddings using the same SentenceTransformer object (`embed_model`). This allows us to compare the input semantically with the knowledge base\n",
        "2. Perform semantic search using util's `semantic_search` function to find the top_k most similar records from **corpus_embeddings**\n",
        "3. Retrieve the actual text from the original corpus by matching the index\n",
        "4. Create an effective and descriptive prompt for the LLM\n",
        "5. Finally, pass the prompt to the Llama-2 function we defined above"
      ],
      "metadata": {
        "id": "sC1PvVCKQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_response(user_input):  # Fixed function name\n",
        "    # Embedding and semantic search\n",
        "    query_embedding = embed_model.encode(user_input, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)\n",
        "    retrieved_contexts = [corpus[hit['corpus_id']] for hit in hits[0]]\n",
        "\n",
        "    # Build prompt for the LLM\n",
        "    context = \"\\n\\n\".join(retrieved_contexts)\n",
        "    prompt = (\n",
        "        \"You are a medical assistant. Based on the medical records below, \"\n",
        "        \"suggest the top 2 possible diseases for these symptoms. Be concise and use bullet points.\\n\"\n",
        "        f\"Medical Records:\\n{context}\\n\\n\"\n",
        "        f\"Symptoms: {user_input}\\n\\n\"\n",
        "        \"Note: This is not a medical diagnosis. Always consult a licensed physician.\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nRETRIEVED CONTEXTS:\", retrieved_contexts)\n",
        "    print(\"\\nPROMPT SENT TO LLM:\\n\", prompt)\n",
        "\n",
        "    # Generate answer - FIXED FUNCTION CALL\n",
        "    return generate_llama2_response(prompt)  # Fixed function name\n"
      ],
      "metadata": {
        "id": "HMZKanYnNbG9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "WpWKIE4GQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # CORRECTED FUNCTION CALL\n",
        "        response = rag_response(user_input)  # Fixed function name\n",
        "\n",
        "        print(f\"ChatBot: {response}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "QK0oEb7KOEUJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The following cell may take some time to run because of embeddings generation and semantic search"
      ],
      "metadata": {
        "id": "cKvWJ7MaQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "vnx0IBpwSnOC",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your Understanding!\n",
        "\n",
        "Time to try fine-tuning an LLM by yourself!  \n",
        "Let's use the BioMistral model once again since it is well-suited for medical applications.   \n",
        "We already have our formatted dataframe, so we will start off by loading the model."
      ],
      "metadata": {
        "id": "DnbNiIw3Q-8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step A: Load `BioMistral/BioMistral-7B` and its tokenizer\n",
        "\n",
        "Refer to Step 5 if you get stuck!"
      ],
      "metadata": {
        "id": "mQBOxW3VQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BioMistral model and tokenizer\n",
        "model_name = \"BioMistral/BioMistral-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model with appropriate settings\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"BioMistral model and tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "liGsg-NNQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step B: Generate a response from the Mistral model\n",
        "\n",
        "Refer to Step 6 if you get stuck!"
      ],
      "metadata": {
        "id": "NnVV3HC5Q-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(prompt):\n",
        "    # Tokenize the input - FIXED: use input_ids instead of input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "\n",
        "    # Generate response\n",
        "    output = llm.generate(\n",
        "        inputs.input_ids,  # Use input_ids instead of just inputs\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return response\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "print(\"Mistral response function defined!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "L6R1w_YyQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step C: Generate a response based on our dataset using RAG\n",
        "\n",
        "Refer to Step 7 if you get stuck!"
      ],
      "metadata": {
        "id": "Cxmoe9dAQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_response(user_input):\n",
        "    # Convert user query to embeddings\n",
        "    query_embedding = embed_model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "    # Perform semantic search\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)\n",
        "    retrieved_contexts = [corpus[hit[\"corpus_id\"]] for hit in hits[0]]\n",
        "\n",
        "    # Build the prompt for BioMistral\n",
        "    prompt = (\n",
        "        \"You are a medical assistant. Based on the medical records below, \"\n",
        "        \"suggest top 2 possible diseases the user might have. Be concise and give the response in points.\\n\"\n",
        "        \"Make sure to also include a disclaimer at the bottom telling users that this is not a medical diagnosis and they should always consult a doctor.\\n\\n\"\n",
        "        \"Medical Records:\\n\" + \"\\n\".join(retrieved_contexts) +\n",
        "        f\"\\n\\nUser Symptoms: {user_input}\\n\\nYour Response:\"\n",
        "    )\n",
        "\n",
        "    return generate_mistral_response(prompt)\n",
        "\n",
        "print(\"RAG response function defined!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "d6zQ-uwuQ-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step D: Create a chatbot interface for the user\n",
        "\n",
        "Refer to Step 8 if you get stuck!"
      ],
      "metadata": {
        "id": "RBfJaeDVQ-8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        response = rag_response(user_input)\n",
        "        print(f\"ChatBot: {response}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")\n",
        "\n",
        "print(\"Chatbot interface defined!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "kkTY8Pm4Q-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "T5C_LA1JlBOp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of RAG + Embeddings\n",
        "* Embedding generation, semantic search and LLM inference are resource-intensive and require longer compute times\n",
        "* Requires GPU for efficieny"
      ],
      "metadata": {
        "id": "WYDXUjK-Q-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "ORM-pRzuQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Note on PEFT and Quantization in Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "sDB_zETbQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter-Efficient Fine-Tuning (PEFT)**  \n",
        "PEFT techniques allow you to fine-tune LLMs by updating only a small subset of parameters rather than the entire model. This makes training more efficient and reduces hardware requirements, ideal when working with limited resources.  \n",
        "\n",
        "**Quantization**  \n",
        "Quantization means converting model weights from a high-memory format (like 32-bit floats) to a lower one (like 8-bit integers). This helps reduce memory usage and allows large models to run on devices with less RAM and smaller GPUs. It also makes inference faster. For example, models can be run on phones or laptops instead of needing expensive servers.  \n",
        "\n",
        "**LoRA (Low-Rank Adaptation)**  \n",
        "LoRA is a technique used during fine-tuning that avoids updating all of the model's weights. Instead, it learns small changes to the model and stores them separately. These changes are computed using two smaller matrices, which means fewer parameters need to be updated. This makes training much faster and lighter.  \n",
        "\n",
        "**QLoRA**  \n",
        "QLoRA combines quantization and LoRA. It compresses model weights to 4-bit precision and then fine-tunes the model using LoRA. This lets you fine-tune large models using much less memory without sacrificing too much performance"
      ],
      "metadata": {
        "id": "9Hard2RiQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next milestone, we will look into implementing QLoRA to fine-tune Llama-2 effectively to meet the GPU constraints"
      ],
      "metadata": {
        "id": "afM6jDR9Q-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "JjwZBqyRQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 4: Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "VSQJKAVdUeVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does fine-tuning LLMs mean?**\n",
        "* Fine-tuning means adapting a pre-trained LLM to perform better on a specific task by continuing its training on a domain-specific dataset\n",
        "* The LLM learns patterns in the dataset and adjusts its internal weights slightly to adapt to that domain to get more relevant responses\n",
        "* For example, in our case, a base model like Llama-2 may just know general health facts but after fine-tuning it on our disease-symptom dataset, it will give more accurate answers\n",
        "\n",
        "In this section of the project, we will be fine-tuning LLMs for our medical chatbot"
      ],
      "metadata": {
        "id": "PAdekVRIQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over Embeddings + RAG Method\n",
        "* Better Domain Alignment: Fine-tuning tailors the model to specifc domain knowledge improving accuracy\n",
        "* Faster Inference: Without a retrieval step, fine-tuned models can respond faster"
      ],
      "metadata": {
        "id": "VDzmg5OcQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install required libraries\n",
        "\n",
        "**peft:** enables parameter-efficient training for large models  \n",
        "**datasets:** used to convert a pandas dataframe into a format that is compatible with Hugging Face's Trainer  \n",
        "**accelerate:** simeplifies mixed-precision training  \n",
        "**bitsandbytes:** enables quantization to reduce memory usage when training large models"
      ],
      "metadata": {
        "id": "764L_7oVQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Before running the cell below, please restart the session. You can do this by clicking the 3 dots on the upper right-hand corner and selecting *Restart & Clear Cell Outputs*. An error message might appear as you run the cell below, but you can carry on with the project without worrying about it!"
      ],
      "metadata": {
        "id": "-Pk7N6Y5Q-8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell only after restarting the session\n",
        "!pip install -q peft datasets accelerate\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "id": "7vQkyzdlQ-8x",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:22.118546Z",
          "iopub.execute_input": "2025-10-06T21:54:22.118929Z",
          "iopub.status.idle": "2025-10-06T21:54:28.721997Z",
          "shell.execute_reply.started": "2025-10-06T21:54:22.118901Z",
          "shell.execute_reply": "2025-10-06T21:54:28.721241Z"
        },
        "outputId": "0e6be5db-680b-4358-d4c8-17a1c18c54a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Note:** Since the session has restarted, the dataset is no longer available. Please return to Milestone 1, run all the cells in that section to reload the data, and then come back to Milestone 4 once you are done!"
      ],
      "metadata": {
        "id": "KjHxNjTTQ-8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**TrainingArguments:** specifies training parameters for the LLM  \n",
        "**Trainer:** training loop abstraction to simplify model training  \n",
        "**BitsAndBytesConfig:** used for quantized training  \n",
        "**LoraConfig:** defines the configuration for LoRA fine-tuning  \n",
        "**get_peft_model:** wraps a base model with PEFT (LoRA) layers  \n",
        "**prepare_model_for_kbit_training:** prepares a model for 4-bit or 8-bit training  \n",
        "**PeftModel:** to load a LoRA-trained model for inferencing"
      ],
      "metadata": {
        "id": "202X7bz7Q-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft import PeftModel\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "_1hkQAsuQ-8y",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:54:32.738391Z",
          "iopub.execute_input": "2025-10-06T21:54:32.738994Z",
          "iopub.status.idle": "2025-10-06T21:54:32.743344Z",
          "shell.execute_reply.started": "2025-10-06T21:54:32.738964Z",
          "shell.execute_reply": "2025-10-06T21:54:32.742701Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transform the dataset into required format for fine-tuning Llama-2\n",
        "\n",
        "Llama models generally require a specific format as the input which is the `[INST] ... [/INST]` format.  \n",
        "For example, we need to transform our dataset to look like this:\n",
        "`<s>[INST] abdominal pain, fever [\\INST] Appendicitis`  \n",
        "\n",
        "1. Define a function that formats the input passed into the format we discussed above\n",
        "2. Apply the `format_prompt` function to each row of the dataframe and create a new column called **text** that stores the formatted prompt for each row\n",
        "3. Convert the dataframe into a Hugging Face `Dataset` object"
      ],
      "metadata": {
        "id": "ai_tO-AIQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(row):\n",
        "    return f\"<s>[INST] {row['Symptoms']} [/INST] {row['Disease']}\"\n",
        "\n",
        "dis_symp_df[\"text\"] = dis_symp_df.apply(format_prompt, axis=1)\n",
        "\n",
        "formatted_df = Dataset.from_pandas(dis_symp_df[[\"text\"]])"
      ],
      "metadata": {
        "id": "gyCd86rNUh8y",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:55:06.438318Z",
          "iopub.execute_input": "2025-10-06T21:55:06.43859Z",
          "iopub.status.idle": "2025-10-06T21:55:06.467348Z",
          "shell.execute_reply.started": "2025-10-06T21:55:06.438573Z",
          "shell.execute_reply": "2025-10-06T21:55:06.466525Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out how the first entry of `formatted_df` looks"
      ],
      "metadata": {
        "id": "2igRNRbVQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_df[0]"
      ],
      "metadata": {
        "id": "Lss2W1h2dmud",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:55:08.989462Z",
          "iopub.execute_input": "2025-10-06T21:55:08.989755Z",
          "iopub.status.idle": "2025-10-06T21:55:08.997612Z",
          "shell.execute_reply.started": "2025-10-06T21:55:08.989727Z",
          "shell.execute_reply": "2025-10-06T21:55:08.996951Z"
        },
        "outputId": "c63a1a44-4668-48dd-ffd8-49496495c566"
      },
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'text': '<s>[INST]  muscle wasting,  patches in throat,  high fever,  extra marital contacts [/INST] AIDS'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Load the Llama-2 chat model using QLoRA\n",
        "\n",
        "1. Specify the Hugging Face model we want to load. Here we will be using `NousResearch/Llama-2-7b-chat-hf` which is a 7B parameter version of Llama-2\n",
        "2. Set up the 4-bit quantization for QLoRA using `BitsAndBytesConfig` and define the parameter values\n",
        "3. Load the quantized model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "sSx0EnwwQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the Hugging Face model we want to load\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Set up the 4-bit quantization for QLoRA using BitsAndBytesConfig\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the quantized model using AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},  # explicitly use GPU 0 (GPU T4 x2)\n",
        ")\n",
        "\n",
        "print(\"Llama-2-7b-chat model loaded successfully with 4-bit quantization!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "U_sXc_hdQ-8z",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:55:09.540708Z",
          "iopub.execute_input": "2025-10-06T21:55:09.541333Z",
          "iopub.status.idle": "2025-10-06T21:57:38.838732Z",
          "shell.execute_reply.started": "2025-10-06T21:55:09.541305Z",
          "shell.execute_reply": "2025-10-06T21:57:38.837959Z"
        },
        "outputId": "3dfb46a0-c7a0-42ba-a6e0-978285c819eb",
        "colab": {
          "referenced_widgets": [
            "0292708fa7eb4905b2794bb0cd98d0b4",
            "bf763d3980e94c1ab33a584bc9162df5",
            "df172a287f074e089e41a06da9e38b08",
            "33b95c592cb74ae2bed1fe12924512b5",
            "e1d849746ee9406f972ebe4ef0c4f51c",
            "bc09fda4f8514be0977e2403965d7e88",
            "8b0091ccad644465967eea8bf8da0555"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0292708fa7eb4905b2794bb0cd98d0b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf763d3980e94c1ab33a584bc9162df5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df172a287f074e089e41a06da9e38b08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33b95c592cb74ae2bed1fe12924512b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1d849746ee9406f972ebe4ef0c4f51c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc09fda4f8514be0977e2403965d7e88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b0091ccad644465967eea8bf8da0555"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Llama-2-7b-chat model loaded successfully with 4-bit quantization!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Set up LoRA for a 4-bit quantized Llama-2 model\n",
        "\n",
        "1. Prepare the 4-bit quantized model for training using `prepare_model_for_kbit_training`\n",
        "2. Create the configuration for LoRA and define the parameter values\n",
        "3. Wrap the model with LoRA using the defined configuration. This resuts in only a small set of trainable weights which reduces compute and memory needs"
      ],
      "metadata": {
        "id": "HuYtRf1zQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the 4-bit quantized model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Create the configuration for LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # rank of the LoRA update matrices\n",
        "    lora_alpha=16, # scaling factor for the LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # apply LoRA only to the query and value projection layers of attention\n",
        "    lora_dropout=0.1, # dropout applied to LoRA layers during training to avoid overfitting\n",
        "    bias=\"none\", # do not train the bias parameters\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA using the defined configuration\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"LoRA configuration applied successfully!\")\n",
        "print(\"Trainable parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUf-azeyQ-8z",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:38.840119Z",
          "iopub.execute_input": "2025-10-06T21:57:38.840537Z",
          "iopub.status.idle": "2025-10-06T21:57:39.022621Z",
          "shell.execute_reply.started": "2025-10-06T21:57:38.840515Z",
          "shell.execute_reply": "2025-10-06T21:57:39.021848Z"
        },
        "outputId": "c7baf202-fdab-4c77-8526-57bd957b8574"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "LoRA configuration applied successfully!\nTrainable parameters:\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Tokenize Dataset\n",
        "\n",
        "1. Load the corresponding tokenizer for our model\n",
        "2. Set the `pad_token` to be the same as the `eos_token` since models like Llama do not have separate padding token defined by default"
      ],
      "metadata": {
        "id": "AcekV-2SQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the corresponding tokenizer for our model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the pad_token to be the same as the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ww85KMVdQ-8z",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:39.023446Z",
          "iopub.execute_input": "2025-10-06T21:57:39.023719Z",
          "iopub.status.idle": "2025-10-06T21:57:40.512541Z",
          "shell.execute_reply.started": "2025-10-06T21:57:39.023694Z",
          "shell.execute_reply": "2025-10-06T21:57:40.511701Z"
        },
        "outputId": "474b3a68-01ba-4cc9-ff84-70128f7cc447",
        "colab": {
          "referenced_widgets": [
            "63d8a0f2b22d4ce0bd204c7dd3c4baa7",
            "1e66cf98a58f4dc68266f6ee1a98fb90",
            "ac12a214ea6f4a10a3ea05b9cac2d9b3",
            "49ea8d6f901344fcbda60b8d731a76ac",
            "d062af72d6fe4361a598d27a05bb68cd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63d8a0f2b22d4ce0bd204c7dd3c4baa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e66cf98a58f4dc68266f6ee1a98fb90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac12a214ea6f4a10a3ea05b9cac2d9b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49ea8d6f901344fcbda60b8d731a76ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d062af72d6fe4361a598d27a05bb68cd"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define function that takes one row and processes it for training\n",
        "4. Using `tokenizer` convert the input text into token IDs\n",
        "5. Set labels to be a copy of input_ids. In causal language modeling, the model is trained to predict the next token so the input and out are the same"
      ],
      "metadata": {
        "id": "WNcE7qLlQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's check what columns are actually in our formatted dataset\n",
        "print(\"Columns in formatted_df:\", formatted_df.column_names)\n",
        "\n",
        "# Format the dataset for BioMistral\n",
        "def format_prompt(row):\n",
        "    return f\"<s>[INST] Symptoms: {row['Symptoms']} [/INST] Possible Disease: {row['Disease']} </s>\"\n",
        "\n",
        "# We need to apply the formatting to the original dataframe first\n",
        "dis_symp_df[\"text\"] = dis_symp_df.apply(format_prompt, axis=1)\n",
        "\n",
        "# Now create the dataset with all columns\n",
        "formatted_df = Dataset.from_pandas(dis_symp_df)\n",
        "\n",
        "print(\"Columns after recreating formatted_df:\", formatted_df.column_names)\n",
        "print(\"\\nFirst formatted example:\")\n",
        "print(formatted_df[0][\"text\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.514102Z",
          "iopub.execute_input": "2025-10-06T21:57:40.514352Z",
          "iopub.status.idle": "2025-10-06T21:57:40.533419Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.514332Z",
          "shell.execute_reply": "2025-10-06T21:57:40.532765Z"
        },
        "id": "_J30q55UlBOs",
        "outputId": "398c149c-f915-45da-e13c-f764b0f2f5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Columns in formatted_df: ['text']\nColumns after recreating formatted_df: ['Disease', 'Symptoms', 'text']\n\nFirst formatted example:\n<s>[INST] Symptoms:  muscle wasting,  patches in throat,  high fever,  extra marital contacts [/INST] Possible Disease: AIDS </s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "trusted": true,
        "id": "jwt1-Y_lQ-80",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.534124Z",
          "iopub.execute_input": "2025-10-06T21:57:40.534414Z",
          "iopub.status.idle": "2025-10-06T21:57:40.549282Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.534393Z",
          "shell.execute_reply": "2025-10-06T21:57:40.548535Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Apply the `tokenize_function` to each row of the `formatted_df` and remove columns **text**, **Disease** and **Symptoms** to keep only the tokenized inputs"
      ],
      "metadata": {
        "id": "-Q8d04FCQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenize_function to each row and remove unnecessary columns\n",
        "tokenized_datasets = formatted_df.map(tokenize_function, remove_columns=[\"text\", \"Disease\", \"Symptoms\"])\n",
        "\n",
        "print(\"Dataset tokenized successfully!\")\n",
        "print(f\"Tokenized dataset size: {len(tokenized_datasets)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ClkRdHSCQ-80",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.550018Z",
          "iopub.execute_input": "2025-10-06T21:57:40.550255Z",
          "iopub.status.idle": "2025-10-06T21:57:40.77355Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.550233Z",
          "shell.execute_reply": "2025-10-06T21:57:40.772747Z"
        },
        "outputId": "76766656-69d9-4051-ba11-7bd23bb6f401",
        "colab": {
          "referenced_widgets": [
            "0292efe7913b42809e94a7215ca7e664"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/313 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0292efe7913b42809e94a7215ca7e664"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset tokenized successfully!\nTokenized dataset size: 313\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Define training parameters for fine-tuning the Llama-2 model"
      ],
      "metadata": {
        "id": "NaUB6C6dQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"llama2-finetune\",\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    max_steps=-1,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    save_steps=0\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "nxcXrvOKQ-80",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.774468Z",
          "iopub.execute_input": "2025-10-06T21:57:40.774721Z",
          "iopub.status.idle": "2025-10-06T21:57:40.826699Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.774696Z",
          "shell.execute_reply": "2025-10-06T21:57:40.826139Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Initialize `Trainer`\n",
        "\n",
        "Define the following parameters:  \n",
        "**model:** the LoRA-wrapped Llama-2 model we are fine-tuning  \n",
        "**args:** the training arguments we defined above  \n",
        "**train_dataset:** the tokenized dataset that contains the formatted and encoded input-output pairs  \n",
        "**tokenizer:** the tokenizer used to process inputs and decode outputs to ensure consistency between training and generation"
      ],
      "metadata": {
        "id": "0BKAyL0LQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YBLRr4V0Q-80",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.827341Z",
          "iopub.execute_input": "2025-10-06T21:57:40.82758Z",
          "iopub.status.idle": "2025-10-06T21:57:40.867884Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.827553Z",
          "shell.execute_reply": "2025-10-06T21:57:40.867376Z"
        },
        "outputId": "360aab47-8af8-49a3-d511-9608534d73a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 9: Train your LLM\n",
        "\n",
        "Finally, after the preprocessing and parameter definition, we can train our LLM!"
      ],
      "metadata": {
        "id": "LIVCm3w4Q-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "qR89mmpsQ-81",
        "execution": {
          "iopub.status.busy": "2025-10-06T21:57:40.868522Z",
          "iopub.execute_input": "2025-10-06T21:57:40.868711Z",
          "iopub.status.idle": "2025-10-06T22:00:21.590558Z",
          "shell.execute_reply.started": "2025-10-06T21:57:40.868696Z",
          "shell.execute_reply": "2025-10-06T22:00:21.589766Z"
        },
        "outputId": "f94b4361-b7a8-4f8d-c463-a8013f86da57"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Starting training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 02:34, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.441100</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.590000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.499700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.980400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.806700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.150900</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.365500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.949100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.760300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.568900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.499600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.474400</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.467900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.434800</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.449600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.421400</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.371600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.364600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.382400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.391800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.330800</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.353900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.332800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.308000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.285000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.296900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.263900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.300900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.286700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.284400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.286600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.333100</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.276400</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.248500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.266100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.269800</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.254200</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.259400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.459600</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training completed!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 10: Model Inferencing\n",
        "\n",
        "Now that we have our fine-tuned LLM, we will use it to predict possible diseases for different user inputs.\n",
        "\n",
        "1. Save the fine-tuned Llama-2 model and tokenizer to a specific directory in the Kaggle environment"
      ],
      "metadata": {
        "id": "9ADxb4OXQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned Llama-2 model and tokenizer\n",
        "output_dir = \"/kaggle/working/llama2-med-chatbot\"\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "WOx1oDLZQ-81",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:21.59248Z",
          "iopub.execute_input": "2025-10-06T22:00:21.592722Z",
          "iopub.status.idle": "2025-10-06T22:00:21.870241Z",
          "shell.execute_reply.started": "2025-10-06T22:00:21.592692Z",
          "shell.execute_reply": "2025-10-06T22:00:21.869434Z"
        },
        "outputId": "82946fa3-49df-4b0d-c643-d1efbc20c3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model and tokenizer saved to: /kaggle/working/llama2-med-chatbot\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the base model, calling the original Llama-2 model `NousResearch/Llama-2-7b-chat-hf`\n",
        "3. Load the tokenizer from `output_dir`, set `pad_token` to `eos_token` and set `padding_side` to right which is standard for causal language models"
      ],
      "metadata": {
        "id": "Lr7JZa0gQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base model\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load the tokenizer from output_dir\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "trusted": true,
        "id": "mH96P718Q-81",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:21.870978Z",
          "iopub.execute_input": "2025-10-06T22:00:21.871236Z",
          "iopub.status.idle": "2025-10-06T22:00:22.002657Z",
          "shell.execute_reply.started": "2025-10-06T22:00:21.871217Z",
          "shell.execute_reply": "2025-10-06T22:00:22.001998Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load the base model with quantization using the bitsandbytes configuration define above"
      ],
      "metadata": {
        "id": "b3_LiqwbQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model with quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8ZboWzGJQ-82",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:22.003442Z",
          "iopub.execute_input": "2025-10-06T22:00:22.003759Z",
          "iopub.status.idle": "2025-10-06T22:00:48.1689Z",
          "shell.execute_reply.started": "2025-10-06T22:00:22.003727Z",
          "shell.execute_reply": "2025-10-06T22:00:48.16835Z"
        },
        "outputId": "53091762-ce63-4db9-929b-de2d1eca7250",
        "colab": {
          "referenced_widgets": [
            "7f9dc820e7a1462f84b2fab11f3ab5ae"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f9dc820e7a1462f84b2fab11f3ab5ae"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Attach the LoRA fine-tuned weights from `output_dir` and merge them with the base model using `PeftModel`\n",
        "6. Set the model to evaluation using the `eval` function to put the model in inference mode"
      ],
      "metadata": {
        "id": "8P0sL-1TQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the LoRA fine-tuned weights and merge them with the base model\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Fine-tuned model loaded for inference!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8XEwOHEZQ-82",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:48.169609Z",
          "iopub.execute_input": "2025-10-06T22:00:48.169804Z",
          "iopub.status.idle": "2025-10-06T22:00:48.328454Z",
          "shell.execute_reply.started": "2025-10-06T22:00:48.169788Z",
          "shell.execute_reply": "2025-10-06T22:00:48.327886Z"
        },
        "outputId": "734183dc-bda6-4136-fe5b-886dc5e7360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Fine-tuned model loaded for inference!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create the chatbot interface function for the user"
      ],
      "metadata": {
        "id": "MMakv1nVQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\nNote: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        instruction = \"List the top 2 possible diseases for these symptoms:\"\n",
        "        # formatting the prompt using the required Llama-2 structure\n",
        "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{instruction}\n",
        "<</SYS>>\n",
        "\n",
        "Symptoms: {user_input} [/INST]\"\"\"\n",
        "\n",
        "        # converts prompt into token IDs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # to generate response from the model with key parameters\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                do_sample=False,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # decode the output tokens into readable text\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # extracts only the relevant part after [/INST] which contains the response\n",
        "        if \"[/INST]\" in full_response:\n",
        "            answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        print(f\"ChatBot: {answer}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "36AKil0eQ-82",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:48.329116Z",
          "iopub.execute_input": "2025-10-06T22:00:48.329391Z",
          "iopub.status.idle": "2025-10-06T22:00:48.335434Z",
          "shell.execute_reply.started": "2025-10-06T22:00:48.329361Z",
          "shell.execute_reply": "2025-10-06T22:00:48.334685Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Jn19r_mnQ-83",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:00:48.33617Z",
          "iopub.execute_input": "2025-10-06T22:00:48.336442Z",
          "iopub.status.idle": "2025-10-06T22:01:27.02879Z",
          "shell.execute_reply.started": "2025-10-06T22:00:48.336418Z",
          "shell.execute_reply": "2025-10-06T22:01:27.027946Z"
        },
        "outputId": "2491fc8b-3a15-4394-803d-781b790eefe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ChatBot: I can help suggest possible diseases based on your symptoms.\nType your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  headache\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Based on the information provided, the top two possible diseases that could be causing the symptom of a headache are:\n\n1. Tension headache: This is the most common type of headache, caused by muscle tension in the neck and scalp.\n2. Migraine: This is a more severe type of headache, characterized by throbbing pain on one side of the head, often accompanied by sensitivity to light and sound, nausea, and vomiting.\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  chills\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Sure, here are two possible diseases that could cause chills:\n\n1. Influenza (flu): Chills are a common symptom of the flu, which is caused by the influenza virus. Other symptoms of the flu include fever, cough, sore throat, and body aches.\n2. Pneumonia: Chills can also be a symptom of pneumonia, which is an infection of the lungs. Other symptoms of pneumonia include cough, fever, chest pain, and difficulty breathing.\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  exit\n"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Goodbye!\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why do you think the fine-tuned LLM may not always give exact responses from the dataframe?\n",
        "\n",
        "**Hint:** Answer along the lines of the generative nature of LLMs, training parameters and sampling parameters (temperature, top_p)"
      ],
      "metadata": {
        "id": "-Mt51h2iQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine-tuned LLM may not always give exact responses from the dataframe due to several factors related to the generative nature of LLMs, training parameters, and inference settings\n",
        "\n",
        "- LLMs learn patterns and relationships in the data rather than memorizing exact text strings. They generate responses based on learned probabilities rather than retrieving exact matches.\n",
        "\n",
        "- Even when fine-tuned, LLMs can paraphrase, rephrase, or generate variations of the training data rather than reproducing it verbatim.\n",
        "\n",
        "- The model understands the semantic meaning and may express the same concept using different wording."
      ],
      "metadata": {
        "id": "RhP7u23pQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your Understanding!\n",
        "\n",
        "Time to try fine-tuning an LLM by yourself!  \n",
        "BioMistral is a domain-specific version of the Mistral LLM, fine-tuned on biomedical and clinical data. It is designed to perform better on healthcare-related tasks. You can read up more about it in this [link](https://huggingface.co/BioMistral).  \n",
        "We already have our formatted dataframe, so we will start off by loading the model."
      ],
      "metadata": {
        "id": "KMqbyk3wQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step A: Load `BioMistral/BioMistral-7B` using QLoRA\n",
        "\n",
        "Refer to Step 4 if you get stuck!"
      ],
      "metadata": {
        "id": "OH6tKcBiQ-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step A: Load BioMistral/BioMistral-7B using QLoRA\n",
        "\n",
        "# Specify the model name\n",
        "model_name = \"BioMistral/BioMistral-7B\"\n",
        "\n",
        "# Set up the 4-bit quantization for QLoRA\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the quantized model\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"BioMistral-7B model loaded successfully with 4-bit quantization!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "hPhzMdXYQ-83",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:01:34.18762Z",
          "iopub.execute_input": "2025-10-06T22:01:34.188245Z",
          "iopub.status.idle": "2025-10-06T22:03:51.607618Z",
          "shell.execute_reply.started": "2025-10-06T22:01:34.188219Z",
          "shell.execute_reply": "2025-10-06T22:03:51.606937Z"
        },
        "outputId": "f422457d-c5ed-4911-e0a3-47b7c38050a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "BioMistral-7B model loaded successfully with 4-bit quantization!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step B: Set up LoRA for a 4-bit quantized BioMistral model\n",
        "\n",
        "**Hint:** For BioMistral the `target_modules` in `LoraConfig` are different since we are changing the LLM architecture. Define the `target_modules` as `[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]`.  \n",
        "Refer to Step 5 if you get stuck!"
      ],
      "metadata": {
        "id": "TwW0tBHNQ-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step B: Set up LoRA for a 4-bit quantized BioMistral model\n",
        "\n",
        "# Prepare the model for k-bit training\n",
        "llm = prepare_model_for_kbit_training(llm)\n",
        "\n",
        "# Create LoRA configuration with BioMistral-specific target modules\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Different for Mistral architecture\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "llm = get_peft_model(llm, lora_config)\n",
        "\n",
        "print(\"LoRA configuration applied to BioMistral!\")\n",
        "print(\"Trainable parameters:\")\n",
        "llm.print_trainable_parameters()"
      ],
      "metadata": {
        "trusted": true,
        "id": "joWTBCZmQ-83",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:51.612033Z",
          "iopub.execute_input": "2025-10-06T22:03:51.612325Z",
          "iopub.status.idle": "2025-10-06T22:03:51.862525Z",
          "shell.execute_reply.started": "2025-10-06T22:03:51.612307Z",
          "shell.execute_reply": "2025-10-06T22:03:51.861718Z"
        },
        "outputId": "e1fafddc-494e-4bd8-a3e1-0f3663e2f778"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "LoRA configuration applied to BioMistral!\nTrainable parameters:\ntrainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step C: Define the tokenizer for the model and tokenize the dataset\n",
        "\n",
        "Refer to Step 6 if you get stuck!"
      ],
      "metadata": {
        "id": "2tsxtgN3Q-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step C: Define the tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Format the dataset for BioMistral (using appropriate format)\n",
        "def format_prompt(row):\n",
        "    return f\"<s>[INST] Symptoms: {row['Symptoms']} [/INST] Possible Disease: {row['Disease']} </s>\"\n",
        "\n",
        "dis_symp_df[\"text\"] = dis_symp_df.apply(format_prompt, axis=1)\n",
        "formatted_df = Dataset.from_pandas(dis_symp_df)\n",
        "\n",
        "print(\"First formatted example:\")\n",
        "print(formatted_df[0][\"text\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "UezR6lOgQ-84",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:51.86338Z",
          "iopub.execute_input": "2025-10-06T22:03:51.863661Z",
          "iopub.status.idle": "2025-10-06T22:03:52.212246Z",
          "shell.execute_reply.started": "2025-10-06T22:03:51.863634Z",
          "shell.execute_reply": "2025-10-06T22:03:52.21157Z"
        },
        "outputId": "3d3b61b9-1ef0-4194-88d8-6e6463e3aa0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "First formatted example:\n<s>[INST] Symptoms:  muscle wasting,  patches in throat,  high fever,  extra marital contacts [/INST] Possible Disease: AIDS </s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(example):\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_datasets = formatted_df.map(tokenize_function, remove_columns=[\"text\", \"Disease\", \"Symptoms\"])\n",
        "\n",
        "print(\"Dataset tokenized successfully!\")\n",
        "print(f\"Tokenized dataset size: {len(tokenized_datasets)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ylQl4LOXQ-84",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:52.21391Z",
          "iopub.execute_input": "2025-10-06T22:03:52.21413Z",
          "iopub.status.idle": "2025-10-06T22:03:52.416584Z",
          "shell.execute_reply.started": "2025-10-06T22:03:52.214112Z",
          "shell.execute_reply": "2025-10-06T22:03:52.416008Z"
        },
        "outputId": "c3c7edbb-8c59-4bc0-fc8b-764f2a5138e9",
        "colab": {
          "referenced_widgets": [
            "58929af91a374553a6e4cd7f1446b13d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/313 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58929af91a374553a6e4cd7f1446b13d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset tokenized successfully!\nTokenized dataset size: 313\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step D: Define training parameters for fine-tuning\n",
        "\n",
        "**Hint:** Use the same parameters as in Step 7"
      ],
      "metadata": {
        "id": "zkQzCAPCQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step D: Define training parameters for fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results-biomistral\",\n",
        "    run_name=\"biomistral-finetune\",\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,  # Reduced batch size for BioMistral\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    save_steps=0,\n",
        "    fp16=True  # Enable mixed precision\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined for BioMistral!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "xq7dfExkQ-84",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:52.417193Z",
          "iopub.execute_input": "2025-10-06T22:03:52.417392Z",
          "iopub.status.idle": "2025-10-06T22:03:52.587706Z",
          "shell.execute_reply.started": "2025-10-06T22:03:52.417377Z",
          "shell.execute_reply": "2025-10-06T22:03:52.586949Z"
        },
        "outputId": "f22e95e1-fb5f-431a-8b17-acd52095f202"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training arguments defined for BioMistral!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step E: Initialize Trainer"
      ],
      "metadata": {
        "id": "4FTSBbksQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step E: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=llm,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"BioMistral Trainer initialized successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "DMmwkZjyQ-84",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:52.588489Z",
          "iopub.execute_input": "2025-10-06T22:03:52.588766Z",
          "iopub.status.idle": "2025-10-06T22:03:52.635887Z",
          "shell.execute_reply.started": "2025-10-06T22:03:52.588741Z",
          "shell.execute_reply": "2025-10-06T22:03:52.635321Z"
        },
        "outputId": "0aa7430d-f029-4487-ad1e-df0227eb312f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "BioMistral Trainer initialized successfully!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step F: Train your LLM"
      ],
      "metadata": {
        "id": "SLtTDHFCQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step F: Train your LLM\n",
        "print(\"Starting BioMistral training...\")\n",
        "trainer.train()\n",
        "print(\"BioMistral training completed!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "HhT8wR6KQ-85",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:03:52.636524Z",
          "iopub.execute_input": "2025-10-06T22:03:52.636705Z",
          "iopub.status.idle": "2025-10-06T22:07:53.705848Z",
          "shell.execute_reply.started": "2025-10-06T22:03:52.636683Z",
          "shell.execute_reply": "2025-10-06T22:07:53.705248Z"
        },
        "outputId": "1d6dee7a-1564-4d1c-a9a3-55b86e991c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Starting BioMistral training...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 03:54, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>9.729500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.274500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5.097100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.878300</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.664400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.420700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.375600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.341300</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.838500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.293200</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.307300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.253000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.252300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.227000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.264600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.229900</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.211000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.206500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.220800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.213400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.178100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.182600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.189400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.151700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.131000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.146700</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.154600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.132300</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.160700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.155000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.147000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.131600</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.139300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.140600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.133000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.177800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.113700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.190900</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "BioMistral training completed!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step G: Model Inferencing"
      ],
      "metadata": {
        "id": "eXHWGENRQ-85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step G: Model Inferencing - Save the model\n",
        "output_dir = \"/kaggle/working/biomistral-chatbot\"\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MhZhYHHtQ-85",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:07:53.707598Z",
          "iopub.execute_input": "2025-10-06T22:07:53.70801Z",
          "iopub.status.idle": "2025-10-06T22:07:54.078058Z",
          "shell.execute_reply.started": "2025-10-06T22:07:53.707989Z",
          "shell.execute_reply": "2025-10-06T22:07:54.077257Z"
        },
        "outputId": "af097c09-3f0c-4906-f35e-71c072abc77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model and tokenizer saved to: /kaggle/working/biomistral-chatbot\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model for inference\n",
        "output_dir = \"/kaggle/working/biomistral-chatbot\"\n",
        "base_model_name = \"BioMistral/BioMistral-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "model.eval()\n",
        "\n",
        "print(\"Fine-tuned BioMistral model loaded for inference!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "AQH61H8tQ-85",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:07:54.079137Z",
          "iopub.execute_input": "2025-10-06T22:07:54.079442Z",
          "iopub.status.idle": "2025-10-06T22:09:21.319632Z",
          "shell.execute_reply.started": "2025-10-06T22:07:54.079415Z",
          "shell.execute_reply": "2025-10-06T22:09:21.318825Z"
        },
        "outputId": "7bf358e4-f4dc-4cff-93a9-5dbe6b52d22c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Fine-tuned BioMistral model loaded for inference!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\nNote: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        instruction = \"List the top 2 possible diseases for these symptoms:\"\n",
        "        # formatting the prompt using the required Llama-2 structure\n",
        "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{instruction}\n",
        "<</SYS>>\n",
        "\n",
        "Symptoms: {user_input} [/INST]\"\"\"\n",
        "\n",
        "        # converts prompt into token IDs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # to generate response from the model with key parameters\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                do_sample=False,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # decode the output tokens into readable text\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # extracts only the relevant part after [/INST] which contains the response\n",
        "        if \"[/INST]\" in full_response:\n",
        "            answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        print(f\"ChatBot: {answer}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "T67XC9xRQ-85",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:09:21.32742Z",
          "iopub.execute_input": "2025-10-06T22:09:21.327661Z",
          "iopub.status.idle": "2025-10-06T22:09:21.33343Z",
          "shell.execute_reply.started": "2025-10-06T22:09:21.327643Z",
          "shell.execute_reply": "2025-10-06T22:09:21.332757Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pwC6SWEkQ-85",
        "execution": {
          "iopub.status.busy": "2025-10-06T22:09:31.073946Z",
          "iopub.execute_input": "2025-10-06T22:09:31.074255Z",
          "iopub.status.idle": "2025-10-06T22:10:44.819065Z",
          "shell.execute_reply.started": "2025-10-06T22:09:31.074222Z",
          "shell.execute_reply": "2025-10-06T22:10:44.818434Z"
        },
        "outputId": "a2e2a9ab-de33-481c-d79f-eef0bbb7105e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ChatBot: I can help suggest possible diseases based on your symptoms.\nType your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  sore throat\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Possible Diseases: 1. Influenza 2. Strep throat\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  tummy ache\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Possible Diseases:\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  stomach ache\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Possible Diseases:\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  headache\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Possible Diseases: Migraine, Sinusitis\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  running nose\n"
        },
        {
          "name": "stderr",
          "text": "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Possible Diseases: 1. Influenza 2. Common cold\n\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  exit\n"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Goodbye!\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "Congratulations! You have successfully developed your own medical chatbots!  \n",
        "\n",
        "We would once again like to point out that these chatbots were developed solely for learning purposes and should not to be used in case of medical emergencies.\n",
        "\n",
        "To submit your amazing work please follow the steps below:\n",
        "* Rename this notebook to *Chipo Jaya Medical_Chatbot*\n",
        "* Download the notebook\n",
        "* Send your notebook to nsdc@nebigdatahub.org\n",
        "* Once our team receives your submission, you will be awarded with a certificate of completion!\n",
        "\n",
        "Thank you for participating in this project and please reach out to nsdc@nebigdatahub.org in case you have any questions!"
      ],
      "metadata": {
        "id": "9HlbK4iAQ-85"
      }
    }
  ]
}