{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "MediBot",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3892155,
          "sourceType": "datasetVersion",
          "datasetId": 2308952
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chipojaya1/medical_chatbot/blob/main/MediBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "karthikudyawar_disease_symptom_prediction_path = kagglehub.dataset_download('karthikudyawar/disease-symptom-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lWk-EL-WjByY"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSDC Data Science Projects"
      ],
      "metadata": {
        "id": "DTfH0tyAQ-8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developing a Medical Chatbot Using RAG and LLMs\n",
        "\n",
        "## Name: MedBot4U"
      ],
      "metadata": {
        "id": "SjM8rSwyQ-8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "dyyca9d-Q-8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Instructions\n",
        "\n",
        "**Please follow the instructions below before proceeding with the project!**  \n",
        "\n",
        "In this project, we will utilize the GPU provided by Kaggle. The GPU will be used to train and infer LLMs.  \n",
        "To activate the GPU, follow the steps outlined in this [document](https://drive.google.com/file/d/10KHE4eJJKkF9TLxEpSBVKFFqAP41yVh0/view?usp=sharing).  \n",
        "\n",
        "❗️It's important to note that the GPU quota is 30 hours per week. While this is a sizable allocation, it's always a good practice to monitor your usage to ensure you stay within the limit."
      ],
      "metadata": {
        "id": "6fvmzlQxQ-8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "9Vqpu_mOQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **❗️Disclaimer❗️**\n",
        "The chatbot developed in this project is **not a substitute for professional medical diagnosis**. Its responses are generated based solely on the dataset it was trained on, which is limited in scope and not clinically comprehensive. Please do not rely on its outputs for making medical decisions.\n",
        "\n",
        "Always consult a licensed healthcare provider for any health concerns.\n",
        "\n",
        "If you are in an emergency situation, please seek immediate medical attention. You can find a list of emergency contact numbers worldwide [in this link.](https://www.dt.com/ca/wp-content/uploads/2017/03/Global-_911_Emergency-Contacts.pdf)"
      ],
      "metadata": {
        "id": "uU1GgRQbQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "40ot6I86Q-8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Introduction\n",
        "\n",
        "**Motivation:** When people feel unwell, they often search online to understand their symptoms but the information they find can be confusing. Visiting a doctor isn’t always immediately possible, especially in remote areas. A medical chatbot can help bridge this gap by giving quick, easy-to-understand information about possible health conditions. It can guide users to make better decisions about whether to seek medical help, all from the comfort of their home.\n",
        "\n",
        "**What are LLMs?**\n",
        "A Large Language Model (LLM) is an advanced AI model that can understand and generate human-like text. It is trained on a large amount of text data and can answer questions, write content, summarize information and hold conversations.\n",
        "\n",
        "**Why LLMs?**\n",
        "LLMs can understand and generate natural human language, making them ideal for building chatbots. They are capable of handling complex queries, providing detailed responses and adapting to different ways people describe their symptoms."
      ],
      "metadata": {
        "id": "_vRsdJKJQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Outline\n",
        "\n",
        "1. Data Preprocessing\n",
        "2. Implementing a simple **rule-based** chatbot\n",
        "3. Developing a chatbot using **text embeddings** and **RAG**\n",
        "4. **Fine-tuning** LLMs for our specific data and use-case"
      ],
      "metadata": {
        "id": "pTeVP-hWQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "nUcYTQnZQ-8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "7h2FXEcgQ-8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import all the necessary libraries which will be used for data preprocessing as well as moving forward in building the chatbot.\n",
        "\n",
        "The **warnings** library is used to supress unimportant warnings while we run the cells"
      ],
      "metadata": {
        "id": "RRDbRc69Q-8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pupy6zmRQ-8Z",
        "execution": {
          "iopub.status.busy": "2025-10-06T02:53:12.302302Z",
          "iopub.execute_input": "2025-10-06T02:53:12.302573Z",
          "iopub.status.idle": "2025-10-06T02:53:14.042234Z",
          "shell.execute_reply.started": "2025-10-06T02:53:12.302546Z",
          "shell.execute_reply": "2025-10-06T02:53:14.04142Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Reading the data\n",
        "\n",
        "We will import a dataset from Kaggle. This step does not require us to download the dataset and we can directly access it using the /kaggle/input path. For this we will first have to add the dataset to our Kaggle notebook.  \n",
        "\n",
        "**To add the data follow the steps below**:\n",
        "1. Click *Input* on the right menu bar\n",
        "2. Select *+ Add Input*\n",
        "3. Enter this URL in the search bar: https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data\n",
        "4. Click on the *+* icon to add the dataset to the notebook\n",
        "\n",
        "You can explore the data we are using for this project [here](https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data)\n",
        "\n",
        "In this project, we will only use **dataset.csv** which contains the disease and its corresponding symptoms list"
      ],
      "metadata": {
        "id": "S4iyPYVJQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "dis_symp_df = pd.read_csv(\"/kaggle/input/disease-symptom-prediction/dataset.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "16yf0f9rQ-8b",
        "execution": {
          "iopub.status.busy": "2025-10-06T02:53:28.786517Z",
          "iopub.execute_input": "2025-10-06T02:53:28.786785Z",
          "iopub.status.idle": "2025-10-06T02:53:28.818752Z",
          "shell.execute_reply.started": "2025-10-06T02:53:28.786764Z",
          "shell.execute_reply": "2025-10-06T02:53:28.818194Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how the dataset is structured using the pandas `head` function"
      ],
      "metadata": {
        "id": "Zg6sLyfmQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "iMiBd98PawCJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:53:55.100511Z",
          "iopub.execute_input": "2025-10-06T02:53:55.10103Z",
          "iopub.status.idle": "2025-10-06T02:53:55.126462Z",
          "shell.execute_reply.started": "2025-10-06T02:53:55.101006Z",
          "shell.execute_reply": "2025-10-06T02:53:55.125817Z"
        },
        "outputId": "4e175143-0e42-4d5b-8109-93901a703ba4"
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease           Symptom_0           Symptom_1                Symptom_2  \\\n0    AIDS      muscle_wasting   patches_in_throat               high_fever   \n1    AIDS   patches_in_throat          high_fever   extra_marital_contacts   \n2    AIDS      muscle_wasting          high_fever   extra_marital_contacts   \n3    AIDS      muscle_wasting   patches_in_throat   extra_marital_contacts   \n4    AIDS      muscle_wasting   patches_in_throat               high_fever   \n\n                 Symptom_3 Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8  \\\n0   extra_marital_contacts       NaN       NaN       NaN       NaN       NaN   \n1                      NaN       NaN       NaN       NaN       NaN       NaN   \n2                      NaN       NaN       NaN       NaN       NaN       NaN   \n3                      NaN       NaN       NaN       NaN       NaN       NaN   \n4                      NaN       NaN       NaN       NaN       NaN       NaN   \n\n  Symptom_9 Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n0       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n1       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n2       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n3       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n4       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n\n  Symptom_16  \n0        NaN  \n1        NaN  \n2        NaN  \n3        NaN  \n4        NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptom_0</th>\n      <th>Symptom_1</th>\n      <th>Symptom_2</th>\n      <th>Symptom_3</th>\n      <th>Symptom_4</th>\n      <th>Symptom_5</th>\n      <th>Symptom_6</th>\n      <th>Symptom_7</th>\n      <th>Symptom_8</th>\n      <th>Symptom_9</th>\n      <th>Symptom_10</th>\n      <th>Symptom_11</th>\n      <th>Symptom_12</th>\n      <th>Symptom_13</th>\n      <th>Symptom_14</th>\n      <th>Symptom_15</th>\n      <th>Symptom_16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>high_fever</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>extra_marital_contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle_wasting</td>\n      <td>patches_in_throat</td>\n      <td>high_fever</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the symptoms have underscores which need to be removed\n",
        "\n",
        "#### Step 2: Remove Underscores from the Symptoms Text\n",
        "\n",
        "Steps to remove the underscores:\n",
        "1. Find all columns in dis_symp_df where the column names start with \"Symptom\"\n",
        "2. After finding those columns, replace the _ with a blank space"
      ],
      "metadata": {
        "id": "_J7Hcn5FkhPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove underscores from symptoms text\n",
        "symptom_cols = [col for col in dis_symp_df.columns if col.startswith(\"Symptom_\")]\n",
        "dis_symp_df[symptom_cols] = dis_symp_df[symptom_cols].replace(\"_\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "7mLnNQGkcX6P",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:54:02.110884Z",
          "iopub.execute_input": "2025-10-06T02:54:02.111177Z",
          "iopub.status.idle": "2025-10-06T02:54:02.128096Z",
          "shell.execute_reply.started": "2025-10-06T02:54:02.111153Z",
          "shell.execute_reply": "2025-10-06T02:54:02.1276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify if the underscores have been removed by printing the first few entries of the dataframe"
      ],
      "metadata": {
        "id": "EsRk8LsOQ-8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the changes\n",
        "print(\"After removing underscores (first 5 rows):\")\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "VQEE0EAPkX-a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:54:06.345824Z",
          "iopub.execute_input": "2025-10-06T02:54:06.346355Z",
          "iopub.status.idle": "2025-10-06T02:54:06.36015Z",
          "shell.execute_reply.started": "2025-10-06T02:54:06.346328Z",
          "shell.execute_reply": "2025-10-06T02:54:06.359516Z"
        },
        "outputId": "728c8b88-c51b-45ad-a30f-71548122d6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "After removing underscores (first 5 rows):\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease           Symptom_0           Symptom_1                Symptom_2  \\\n0    AIDS      muscle wasting   patches in throat               high fever   \n1    AIDS   patches in throat          high fever   extra marital contacts   \n2    AIDS      muscle wasting          high fever   extra marital contacts   \n3    AIDS      muscle wasting   patches in throat   extra marital contacts   \n4    AIDS      muscle wasting   patches in throat               high fever   \n\n                 Symptom_3 Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8  \\\n0   extra marital contacts       NaN       NaN       NaN       NaN       NaN   \n1                      NaN       NaN       NaN       NaN       NaN       NaN   \n2                      NaN       NaN       NaN       NaN       NaN       NaN   \n3                      NaN       NaN       NaN       NaN       NaN       NaN   \n4                      NaN       NaN       NaN       NaN       NaN       NaN   \n\n  Symptom_9 Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n0       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n1       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n2       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n3       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n4       NaN        NaN        NaN        NaN        NaN        NaN        NaN   \n\n  Symptom_16  \n0        NaN  \n1        NaN  \n2        NaN  \n3        NaN  \n4        NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptom_0</th>\n      <th>Symptom_1</th>\n      <th>Symptom_2</th>\n      <th>Symptom_3</th>\n      <th>Symptom_4</th>\n      <th>Symptom_5</th>\n      <th>Symptom_6</th>\n      <th>Symptom_7</th>\n      <th>Symptom_8</th>\n      <th>Symptom_9</th>\n      <th>Symptom_10</th>\n      <th>Symptom_11</th>\n      <th>Symptom_12</th>\n      <th>Symptom_13</th>\n      <th>Symptom_14</th>\n      <th>Symptom_15</th>\n      <th>Symptom_16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>high fever</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>extra marital contacts</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle wasting</td>\n      <td>patches in throat</td>\n      <td>high fever</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transforming the Structure of the Dataset\n",
        "\n",
        "We need to transform this dataset to match input-output pairs that are suitable for training LLMs\n",
        "\n",
        "Two Methods:\n",
        "\n",
        "\n",
        "1.   Pivot Longer: will result in a bigger dataset and more suited for simple classification tasks\n",
        "2.   Comma-separated symptoms in one column: suitable for sentence-level input from users\n",
        "\n",
        "Therefore, we will proceed with combining the symptoms into one column and separating them with commas\n",
        "\n"
      ],
      "metadata": {
        "id": "734MMipLcMHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to create a comma-separated list of symptoms:\n",
        "\n",
        "\n",
        "1. Combine the symptom values row-wise  \n",
        "For each row, go through the values in the symptom_cols:\n",
        "* Skip any missing values\n",
        "* Join the non-missing symptom strings with commas\n",
        "* Store this in a new column called **Symptoms**\n",
        "\n",
        "2. Remove all the original symptoms columns"
      ],
      "metadata": {
        "id": "OKpEsnqDQ-8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a lambda function to combine the symptom values row-wise  \n",
        "\n",
        "**What is a lambda function?**  \n",
        "`square = lambda x: x*x`  \n",
        "`print(square(5))`\n",
        "\n",
        "`lambda x: x*x` is a lambda function that takes x as input and returns x*x in just one, short line"
      ],
      "metadata": {
        "id": "ORUzm71aQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the symptom values row-wise\n",
        "dis_symp_df[\"Symptoms\"] = dis_symp_df[symptom_cols].apply(lambda row: ', '.join([s for s in row if pd.notna(s)]), axis=1)\n",
        "dis_symp_df.drop(symptom_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "52j6BfrDlMFA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:54:24.727449Z",
          "iopub.execute_input": "2025-10-06T02:54:24.727718Z",
          "iopub.status.idle": "2025-10-06T02:54:24.739112Z",
          "shell.execute_reply.started": "2025-10-06T02:54:24.727697Z",
          "shell.execute_reply": "2025-10-06T02:54:24.738374Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the updated dataframe again using the `head` function"
      ],
      "metadata": {
        "id": "Jk-QM5BLQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame after combining symptoms:\")\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "s9NwCLB9mq20",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:54:40.95203Z",
          "iopub.execute_input": "2025-10-06T02:54:40.952742Z",
          "iopub.status.idle": "2025-10-06T02:54:40.960344Z",
          "shell.execute_reply.started": "2025-10-06T02:54:40.952717Z",
          "shell.execute_reply": "2025-10-06T02:54:40.959689Z"
        },
        "outputId": "0a9dc4f8-68a8-4ecc-d863-46be8b5409d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DataFrame after combining symptoms:\n",
          "output_type": "stream"
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  Disease                                           Symptoms\n0    AIDS   muscle wasting,  patches in throat,  high fev...\n1    AIDS   patches in throat,  high fever,  extra marita...\n2    AIDS   muscle wasting,  high fever,  extra marital c...\n3    AIDS   muscle wasting,  patches in throat,  extra ma...\n4    AIDS    muscle wasting,  patches in throat,  high fever",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  high fev...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AIDS</td>\n      <td>patches in throat,  high fever,  extra marita...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  high fever,  extra marital c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  extra ma...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  high fever</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Check for Duplicate Lists\n",
        "\n",
        "Obtain a summary of the dataset using `info` function"
      ],
      "metadata": {
        "id": "5dstfR8wQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset info before removing duplicates:\")\n",
        "print(dis_symp_df.info())\n",
        "print(f\"\\nNumber of rows before removing duplicates: {len(dis_symp_df)}\")"
      ],
      "metadata": {
        "id": "H7AeYZIgnmnv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:55:27.560579Z",
          "iopub.execute_input": "2025-10-06T02:55:27.561086Z",
          "iopub.status.idle": "2025-10-06T02:55:27.575676Z",
          "shell.execute_reply.started": "2025-10-06T02:55:27.561061Z",
          "shell.execute_reply": "2025-10-06T02:55:27.575034Z"
        },
        "outputId": "0b89d062-93e7-4cc2-b9b5-011f014020b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset info before removing duplicates:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 313 entries, 0 to 312\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Disease   313 non-null    object\n 1   Symptoms  313 non-null    object\ndtypes: object(2)\nmemory usage: 5.0+ KB\nNone\n\nNumber of rows before removing duplicates: 313\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop duplicate columns if they have the same symptoms list  \n",
        "**Hint**: Use the `drop_duplicates` function only for the **Symptoms** column"
      ],
      "metadata": {
        "id": "cwwWxjf-pIGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate columns with the same symptoms list\n",
        "initial_count = len(dis_symp_df)\n",
        "dis_symp_df.drop_duplicates(subset=['Symptoms'], inplace=True)\n",
        "final_count = len(dis_symp_df)"
      ],
      "metadata": {
        "id": "2S3JYj1gnoHX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:55:34.365404Z",
          "iopub.execute_input": "2025-10-06T02:55:34.365686Z",
          "iopub.status.idle": "2025-10-06T02:55:34.370934Z",
          "shell.execute_reply.started": "2025-10-06T02:55:34.365666Z",
          "shell.execute_reply": "2025-10-06T02:55:34.370276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the summary of the dataset again to see if there were any duplicates. Report back on your conclusion."
      ],
      "metadata": {
        "id": "kFegXNooQ-8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nNumber of rows after removing duplicates: {len(dis_symp_df)}\")\n",
        "print(f\"Number of duplicates removed: {initial_count - final_count}\")"
      ],
      "metadata": {
        "id": "HlQ7KxTgo8I6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:55:39.610159Z",
          "iopub.execute_input": "2025-10-06T02:55:39.610429Z",
          "iopub.status.idle": "2025-10-06T02:55:39.615309Z",
          "shell.execute_reply.started": "2025-10-06T02:55:39.610412Z",
          "shell.execute_reply": "2025-10-06T02:55:39.614459Z"
        },
        "outputId": "497a17a6-d108-40f5-8098-9336778bf914"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nNumber of rows after removing duplicates: 313\nNumber of duplicates removed: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final dataframe\n",
        "print(\"\\nFinal DataFrame:\")\n",
        "print(dis_symp_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:55:40.185667Z",
          "iopub.execute_input": "2025-10-06T02:55:40.186328Z",
          "iopub.status.idle": "2025-10-06T02:55:40.192461Z",
          "shell.execute_reply.started": "2025-10-06T02:55:40.186288Z",
          "shell.execute_reply": "2025-10-06T02:55:40.191593Z"
        },
        "id": "zzzURMoYjBye",
        "outputId": "b3756140-41b9-45d0-c5e8-3d811cf19bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nFinal DataFrame:\n  Disease                                           Symptoms\n0    AIDS   muscle wasting,  patches in throat,  high fev...\n1    AIDS   patches in throat,  high fever,  extra marita...\n2    AIDS   muscle wasting,  high fever,  extra marital c...\n3    AIDS   muscle wasting,  patches in throat,  extra ma...\n4    AIDS    muscle wasting,  patches in throat,  high fever\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Your conclusion here***"
      ],
      "metadata": {
        "id": "qOo-3JZxpPGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "NUNwR6pZQ-8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 2: Rule-Based Chatbot (Cosine Similarity)"
      ],
      "metadata": {
        "id": "SeBqC4pl4Ma1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now be implementing one of the most basic versions of a chatbot: **a rule-based chatbot using cosine similarity**.\n",
        "\n",
        "**Cosine similarity** is a metric used to measure how similar two vectors are, regardless of their magnitude.\n",
        "\n",
        "A **rule-based chatbot** using cosine similarity identifies the most appropriate response by comparing the user’s input with a set of predefined statements and selecting the one with the highest semantic similarity based on cosine similarity of their vector embeddings."
      ],
      "metadata": {
        "id": "zKtXgjG7Q-8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Import required packages from `sklearn`\n",
        "\n",
        "`TfidfVectorizer`: Read up more on Term Frequency-Inverse Document Frequency (TF-IDF) [here](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/). This is used to convert text into numerical vectors based on how important each word is.\n",
        "\n",
        "`cosine_similarity`: Package used to measure how similar the user's symptom input is to each disease's symptom list."
      ],
      "metadata": {
        "id": "B5LUmH6zQ-8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the two key tools for comparing text documents\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   # for converting a list of text documents into numerical vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity        # for measuring how similar two sources are based on the words they contain"
      ],
      "metadata": {
        "id": "GHwuLmIK4P7x",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:57:45.633559Z",
          "iopub.execute_input": "2025-10-06T02:57:45.633839Z",
          "iopub.status.idle": "2025-10-06T02:57:46.254087Z",
          "shell.execute_reply.started": "2025-10-06T02:57:45.633818Z",
          "shell.execute_reply": "2025-10-06T02:57:46.253576Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Group All Symptom Entries For Each Disease into a Single String"
      ],
      "metadata": {
        "id": "ytLWF2-AQ-8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** Use a lambda function again. Group by **Disease** and then apply the lambda function to **Symptoms**"
      ],
      "metadata": {
        "id": "QTPfd5nnQ-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group all symptoms for each disease\n",
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(x)).reset_index()"
      ],
      "metadata": {
        "id": "KqjPU44A4VI2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:57:52.152341Z",
          "iopub.execute_input": "2025-10-06T02:57:52.152731Z",
          "iopub.status.idle": "2025-10-06T02:57:52.160085Z",
          "shell.execute_reply.started": "2025-10-06T02:57:52.152712Z",
          "shell.execute_reply": "2025-10-06T02:57:52.159294Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Vectorize only the **Symptoms** column from dis_symp_df using `TfidfVectorizer` and `fit_transform()`"
      ],
      "metadata": {
        "id": "YXkxDB64Q-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize only the Symptoms\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(dis_symp_df[\"Symptoms\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "x0Or1EPCQ-8n",
        "execution": {
          "iopub.status.busy": "2025-10-06T02:58:10.092291Z",
          "iopub.execute_input": "2025-10-06T02:58:10.092576Z",
          "iopub.status.idle": "2025-10-06T02:58:10.109518Z",
          "shell.execute_reply.started": "2025-10-06T02:58:10.092557Z",
          "shell.execute_reply": "2025-10-06T02:58:10.108888Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:58:12.275425Z",
          "iopub.execute_input": "2025-10-06T02:58:12.276201Z",
          "iopub.status.idle": "2025-10-06T02:58:12.28361Z",
          "shell.execute_reply.started": "2025-10-06T02:58:12.27617Z",
          "shell.execute_reply": "2025-10-06T02:58:12.283002Z"
        },
        "id": "Ge_QgZhhjByi",
        "outputId": "7ba795fb-0183-49b7-a62e-2ba4ff6277b5"
      },
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "               Disease                                           Symptoms\n0                 AIDS   muscle wasting,  patches in throat,  high fev...\n1                 Acne   skin rash,  pus filled pimples,  blackheads, ...\n2  Alcoholic hepatitis   vomiting,  yellowish skin,  swelling of stoma...\n3              Allergy   shivering,  chills,  watering from eyes,  con...\n4            Arthritis   muscle weakness,  stiff neck,  swelling joint...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease</th>\n      <th>Symptoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AIDS</td>\n      <td>muscle wasting,  patches in throat,  high fev...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Acne</td>\n      <td>skin rash,  pus filled pimples,  blackheads, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alcoholic hepatitis</td>\n      <td>vomiting,  yellowish skin,  swelling of stoma...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Allergy</td>\n      <td>shivering,  chills,  watering from eyes,  con...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Arthritis</td>\n      <td>muscle weakness,  stiff neck,  swelling joint...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "wZlHWTKQQ-8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    # Welcome message for the user\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Continue asking the user for input until they enter 'exit' or 'quit'\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # Converts the user's input into a TF-IDF vector using the previously trained vectorizer\n",
        "        user_vec = vectorizer.transform([user_input])\n",
        "\n",
        "        # Compares the user's vector with all disease-symptom vectors in the tfidf_matrix using cosine similarity\n",
        "        # flatten() is used to convert the 2D result into a 1D array of vectors\n",
        "        cosine_sim = cosine_similarity(user_vec, tfidf_matrix).flatten()\n",
        "\n",
        "\n",
        "        # Sorts the similarity scores in descending order and retrieves the top 3 indices\n",
        "        top_indices = cosine_sim.argsort()[::-1][:3]\n",
        "\n",
        "        # Creates a list of (disease name, similarity score) tuples and only includes matches where the score is >0.5\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            if cosine_sim[i] > 0.2:\n",
        "                disease = dis_symp_df.iloc[i][\"Disease\"]\n",
        "                score = cosine_sim[i]\n",
        "                results.append(disease)\n",
        "\n",
        "        if not results:\n",
        "            print(\"ChatBot: I couldn not find a good match for your symptoms. Try rephrasing or listing more symptoms.\\n\")\n",
        "            continue\n",
        "\n",
        "        # If there are results, print the top-matching diseases with their similarity scores\n",
        "        print(\"ChatBot: Based on your symptoms, here are possible conditions:\")\n",
        "        for i, (disease) in enumerate(results, 1):\n",
        "            print(f\"   {i}. {disease}\")\n",
        "\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "MfXnIvff4jt6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:58:17.469041Z",
          "iopub.execute_input": "2025-10-06T02:58:17.46937Z",
          "iopub.status.idle": "2025-10-06T02:58:17.475373Z",
          "shell.execute_reply.started": "2025-10-06T02:58:17.469349Z",
          "shell.execute_reply": "2025-10-06T02:58:17.474651Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "tziZARey7iW4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:58:21.556886Z",
          "iopub.execute_input": "2025-10-06T02:58:21.557166Z",
          "iopub.status.idle": "2025-10-06T02:58:41.663179Z",
          "shell.execute_reply.started": "2025-10-06T02:58:21.557145Z",
          "shell.execute_reply": "2025-10-06T02:58:41.662454Z"
        },
        "outputId": "1b6e50f6-a1f5-446b-c9aa-8acd4516f35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ChatBot: I can help suggest possible diseases based on your symptoms.\nType your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  cough\n"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Based on your symptoms, here are possible conditions:\n   1. Covid\n   2. Bronchial Asthma\n   3. GERD\nNote: This is not a medical diagnosis. Always consult a licensed physician.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  exit\n"
        },
        {
          "name": "stdout",
          "text": "ChatBot: Goodbye!\n Note: This is not a medical diagnosis. Always consult a licensed physician.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of Rule-Based Technique\n",
        "\n",
        "\n",
        "*   Does not generalize well to unseen data since there is no training involved\n",
        "*   Not scalable\n",
        "\n"
      ],
      "metadata": {
        "id": "uco9GYd-AIiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "qqj2hXb-Q-8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 3: Embeddings + RAG"
      ],
      "metadata": {
        "id": "uwgAP0vpAUoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Retrieval Augmented Generation (RAG)?**\n",
        "* RAG allows a model to use external data it hasn’t been explicitly trained on\n",
        "* It addresses common LLM limitations like lack of real-time information and outdated knowledge\n",
        "* It works by converting both user queries and a knowledge base into vector embeddings\n",
        "* Uses similarity search to retrieve the most relevant context from the knowledge base\n",
        "* This retrieved context is appended to the user query and passed to the LLM to generate a more accurate and informed response"
      ],
      "metadata": {
        "id": "zuaYgbbUQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings** are numerical representations of text that capture its meaning and semantic similarity in a vector space."
      ],
      "metadata": {
        "id": "QVASENgRQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over the Rule-Based Method\n",
        "* Flexible and Scalable: Unlike TF-IDF which depends on exact word matches, embedding-based retrieval finds relevant records based on context and similarity in meaning\n",
        "* More Robust: Since embeddings capture the semantic meaning behind words and generalize over language structure, minor spelling errors or synonymns do not affect performance, unlike TF-IDF which is sensitive to exact tokens\n",
        "* Context-Aware Responses: RAG combines retrieval with an LLM allowing it to generate human-like responses instead of returning pre-written text\n",
        "* Easier to Update Knowledge: New information can be added to the embedding database without retraining the LLM"
      ],
      "metadata": {
        "id": "VonKsE9WQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the project, we will be implementing a RAG-based chatbot using SentenceTransformer to create embeddings and the Llama-2 LLM to generate responses"
      ],
      "metadata": {
        "id": "KoC4MkBQQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** After running some of these cells, you may get warnings in a red box. Warnings are messages that alert us about possible issues in the code that aren't severe enough to stop execution. This is totally normal and you can still proceed with implementing the chatbot!"
      ],
      "metadata": {
        "id": "py1EuqT7Q-8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install the auto-gptq and optimum libraries\n",
        "\n",
        "**auto-gptq:** used for loading and running quantized versions of large language models efficiently (more on this in Milestone 4)  \n",
        "**optimum:** a library by Hugging Face that helps optimize model inference and training, particularly with quantized models"
      ],
      "metadata": {
        "id": "Lb8IasSXQ-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq\n",
        "!pip install optimum"
      ],
      "metadata": {
        "id": "E1SWpwPmTH1v",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T02:59:04.429052Z",
          "iopub.execute_input": "2025-10-06T02:59:04.42955Z",
          "iopub.status.idle": "2025-10-06T03:00:22.839025Z",
          "shell.execute_reply.started": "2025-10-06T02:59:04.429528Z",
          "shell.execute_reply": "2025-10-06T03:00:22.838095Z"
        },
        "outputId": "369a756f-a1d8-44fd-ba03-13d7d0443038"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting auto-gptq\n  Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.8.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (3.6.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.26.4)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.3.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.6.0+cu124)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.5.3)\nRequirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.52.4)\nRequirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.15.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.67.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.33.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->auto-gptq)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.70.16)\nCollecting fsspec (from torch>=1.13.0->auto-gptq)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0->auto-gptq) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->auto-gptq) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->auto-gptq) (2024.2.0)\nDownloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gekko-1.3.0-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rouge, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, gekko, auto-gptq\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed auto-gptq-0.7.1 fsspec-2025.3.0 gekko-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-1.0.1\nCollecting optimum\n  Downloading optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum) (4.52.4)\nRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum) (2.6.0+cu124)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum) (25.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (2025.3.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optimum) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optimum) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optimum) (2024.2.0)\nDownloading optimum-1.27.0-py3-none-any.whl (425 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: optimum\nSuccessfully installed optimum-1.27.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**torch:** imports PyTorch, a popular deep learning framework used for model loading, tensor computations and training/inference  \n",
        "**transformers:** Hugging Face's library for working with pretrained models  \n",
        "**AutoTokenizer:** automatically loads the appropriate tokenizer for a given model  \n",
        "**AutoModelForCausalLM:** loads a causal language model (used for text generation)  \n",
        "**sentence_transformers:** a library for generating embeddings (vector representations) of sentences  \n",
        "**SentenceTransformer:** Loads a model to convert text into embeddings  \n",
        "**util:** Provides utility functions like `semantic_search()` for comparing embeddings."
      ],
      "metadata": {
        "id": "bEOd5Iz0Q-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "phb243S_AatK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:00:33.645986Z",
          "iopub.execute_input": "2025-10-06T03:00:33.646298Z",
          "iopub.status.idle": "2025-10-06T03:00:59.25487Z",
          "shell.execute_reply.started": "2025-10-06T03:00:33.64627Z",
          "shell.execute_reply": "2025-10-06T03:00:59.254263Z"
        },
        "outputId": "39912322-a63f-45e9-dc2c-2eddde170165"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-06 03:00:45.541879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759719645.707116      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759719645.759078      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Prepare the dataset for embeddings\n",
        "\n",
        "1. Group rows by disease, join all symptoms into one sentence and convert grouped result back into a dataframe using `reset_index()`\n",
        "2. Create a **Text** column with the combined disease and its respective symptom list\n",
        "3. Convert the **Text** column into a list called **corpus** which will be used for generating embeddings. The corpus is our knowledge base."
      ],
      "metadata": {
        "id": "s1_reunRQ-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(x)).reset_index()\n",
        "dis_symp_df[\"Text\"] = dis_symp_df.apply(lambda row: f\"Disease: {row['Disease']}. Symptoms: {row['Symptoms']}\", axis=1)\n",
        "corpus = dis_symp_df[\"Text\"].tolist()"
      ],
      "metadata": {
        "id": "jDOl20HOK1el",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:01:16.83768Z",
          "iopub.execute_input": "2025-10-06T03:01:16.838339Z",
          "iopub.status.idle": "2025-10-06T03:01:16.848327Z",
          "shell.execute_reply.started": "2025-10-06T03:01:16.838314Z",
          "shell.execute_reply": "2025-10-06T03:01:16.847507Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Transform the corpus into vector embeddings\n",
        "\n",
        "1. Load the pre-trained embeddings model `all-MiniLM-L6-v2` from SentenceTransformer\n",
        "2. Convert each text entry in the corpus into its numerical representation using `encode` and set `convert_to_tensor` to **True** to ensure that the output is in the PyTorch tensor format"
      ],
      "metadata": {
        "id": "_UXcnOJ6Q-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = embed_model.encode(corpus, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "EUiEGwdzLYGt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:18:00.953983Z",
          "iopub.execute_input": "2025-10-06T03:18:00.954637Z",
          "iopub.status.idle": "2025-10-06T03:18:02.295566Z",
          "shell.execute_reply.started": "2025-10-06T03:18:00.954611Z",
          "shell.execute_reply": "2025-10-06T03:18:02.294667Z"
        },
        "outputId": "cc41efa3-c81a-46b1-a034-7e4427d40b7a",
        "colab": {
          "referenced_widgets": [
            "dc0968f8e357413c8b3803ae534d6aa1"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc0968f8e357413c8b3803ae534d6aa1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Load the LLM and its tokenizer\n",
        "\n",
        "1. Specify the pre-trained model. Here, we will be using a quantized (compressed) version of Llama-2-7B-Chat model fine-tuned and optimized by the TheBloke using GPTQ. You can read up more about it in this [link](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ).\n",
        "2. Load the corresponding tokenizer using `AutoTokenizer`\n",
        "3. Load the Llama-2 model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "ZDh5wao4Q-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Llama-2?**  \n",
        "LLaMA-2 is a family of open-source LLMs developed by Meta designed for natural language understanding and generation tasks"
      ],
      "metadata": {
        "id": "IYFbPuOgQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model repository name from Hugging Face Hub\n",
        "model_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "# Load the tokenizer for text input processing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "\n",
        "# Load the Llama-2 model (quantized, GPTQ version)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "OkE9uVS7LjsF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:18:07.808536Z",
          "iopub.execute_input": "2025-10-06T03:18:07.809225Z",
          "iopub.status.idle": "2025-10-06T03:18:11.903022Z",
          "shell.execute_reply.started": "2025-10-06T03:18:07.809194Z",
          "shell.execute_reply": "2025-10-06T03:18:11.902231Z"
        },
        "outputId": "d04e25df-c57d-43cb-aa65-f847fd67e237"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at TheBloke/Llama-2-7B-Chat-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Generate a response from the Llama-2 model\n",
        "\n",
        "1. Tokenize the prompt\n",
        "2. Generate the output using sampling paramaters such as `max_new_tokens`, `do_sample`, `temperature` and `top_p`\n",
        "3. Decode the response into a readable string using `decode`\n",
        "4. Remove the original prompt text and return only the generated response"
      ],
      "metadata": {
        "id": "F77YjmJwQ-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Parameters:**  \n",
        "`max_new_tokens`: controls the length of the generated response  \n",
        "`do_sample`: enables sampling, picks the next token randomly based on the predicted probability distribution  \n",
        "`temperature`: a lower value gives a more factual response while a higher value could lead to potential hallucination  \n",
        "`top_p`: a higher value ensures the model avoids rare and low probability words"
      ],
      "metadata": {
        "id": "XZMc28QAQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama2_response(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3nGdPLuIQ-8s",
        "execution": {
          "iopub.status.busy": "2025-10-06T03:18:53.515058Z",
          "iopub.execute_input": "2025-10-06T03:18:53.515372Z",
          "iopub.status.idle": "2025-10-06T03:18:53.520556Z",
          "shell.execute_reply.started": "2025-10-06T03:18:53.515349Z",
          "shell.execute_reply": "2025-10-06T03:18:53.51979Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Generate a response based on our dataset using RAG\n",
        "\n",
        "1. Convert the user query into embeddings using the same SentenceTransformer object (`embed_model`). This allows us to compare the input semantically with the knowledge base\n",
        "2. Perform semantic search using util's `semantic_search` function to find the top_k most similar records from **corpus_embeddings**\n",
        "3. Retrieve the actual text from the original corpus by matching the index\n",
        "4. Create an effective and descriptive prompt for the LLM\n",
        "5. Finally, pass the prompt to the Llama-2 function we defined above"
      ],
      "metadata": {
        "id": "sC1PvVCKQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_response(user_input):\n",
        "    query_embedding = embed_model.encode(user_input, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)\n",
        "    retrieved_contexts = [corpus[hit[\"corpus_id\"]] for hit in hits[0]]\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a medical assistant. Based on the medical records below, \"\n",
        "        f\"suggest top 2 possible diseases the user might have. Be concise and give the response in points.\\n\\n\"\n",
        "        \"Make sure to also include a disclaimer at the bottom telling users that this is not a medical diagnosis and they should always consult a doctor.\"\n",
        "        \"Medical Records:\\n\" + \"\\n\".join(retrieved_contexts) +\n",
        "        f\"\\n\\nUser Symptoms: {user_input}\\n\\nYour Response:\"\n",
        "    )\n",
        "\n",
        "    return generate_llama2_response(prompt)\n",
        ""
      ],
      "metadata": {
        "id": "HMZKanYnNbG9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:19:21.86264Z",
          "iopub.execute_input": "2025-10-06T03:19:21.863274Z",
          "iopub.status.idle": "2025-10-06T03:19:21.868951Z",
          "shell.execute_reply.started": "2025-10-06T03:19:21.863224Z",
          "shell.execute_reply": "2025-10-06T03:19:21.868266Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "WpWKIE4GQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a chatbot interface for the user\n",
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True: # Continue asking the user for input until they enter 'exit' or 'quit'\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # call the rag_response function to obtain the Llama-2 generated output\n",
        "        response = rag_response(user_input)\n",
        "\n",
        "        print(f\"ChatBot: {response}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "QK0oEb7KOEUJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:19:31.861126Z",
          "iopub.execute_input": "2025-10-06T03:19:31.86152Z",
          "iopub.status.idle": "2025-10-06T03:19:31.866152Z",
          "shell.execute_reply.started": "2025-10-06T03:19:31.861493Z",
          "shell.execute_reply": "2025-10-06T03:19:31.865481Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The following cell may take some time to run because of embeddings generation and semantic search"
      ],
      "metadata": {
        "id": "cKvWJ7MaQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "vnx0IBpwSnOC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-06T03:19:34.014928Z",
          "iopub.execute_input": "2025-10-06T03:19:34.015661Z",
          "iopub.status.idle": "2025-10-06T03:21:32.050726Z",
          "shell.execute_reply.started": "2025-10-06T03:19:34.015635Z",
          "shell.execute_reply": "2025-10-06T03:21:32.049628Z"
        },
        "outputId": "c6e52869-2d15-44fd-b0d5-ca7cde12da9a",
        "colab": {
          "referenced_widgets": [
            "0f063562e659446189d3ce6fcd56383a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ChatBot: I can help suggest possible diseases based on your symptoms.\nType your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\n\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "You:  flu\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f063562e659446189d3ce6fcd56383a"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_36/282590679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the chatbot interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_36/2149171741.py\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# call the rag_response function to obtain the Llama-2 generated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ChatBot: {response}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_36/2651492050.py\u001b[0m in \u001b[0;36mrag_response\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_llama2_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_36/3804506020.py\u001b[0m in \u001b[0;36mgenerate_llama2_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_llama2_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3546\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3548\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3549\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3550\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your Understanding!\n",
        "\n",
        "Time to try fine-tuning an LLM by yourself!  \n",
        "Let's use the BioMistral model once again since it is well-suited for medical applications.   \n",
        "We already have our formatted dataframe, so we will start off by loading the model."
      ],
      "metadata": {
        "id": "DnbNiIw3Q-8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step A: Load `BioMistral/BioMistral-7B` and its tokenizer\n",
        "\n",
        "Refer to Step 5 if you get stuck!"
      ],
      "metadata": {
        "id": "mQBOxW3VQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"mistralai/BioMistral-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "liGsg-NNQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the LLaMA model with specified context, GPU layers, and batch size\n",
        "llm = Llama(\n",
        "    model_path=model_path, #Path to the GGUF model file\n",
        "    n_ctx=2300, #Sets the context window to 2300 tokens (how much text the model can \"see\" at once)\n",
        "    n_gpu_layers=38, #Loads 38 model layers onto GPU for faster inference (set to 0 for CPU-only)\n",
        "    n_batch=512 #Number of tokens processed at once\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bxZd_L8IjByl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step B: Generate a response from the Mistral model\n",
        "\n",
        "Refer to Step 6 if you get stuck!"
      ],
      "metadata": {
        "id": "NnVV3HC5Q-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def response(query, max_tokens=512, temperature=0, top_p=0.95, top_k=50):\n",
        "    # Sends the query prompt to the LLM with specified generation parameters\n",
        "    model_output = llm(\n",
        "        prompt=query, #The user's input question or prompt sent to the LLM\n",
        "        max_tokens=max_tokens, #Maximum number of tokens to generate\n",
        "        temperature=temperature, #Controls randomness\n",
        "        top_p=top_p, #picks from top tokens that make up top_p of total probability\n",
        "        top_k=top_k #considers only the top_k most likely tokens\n",
        "    )\n",
        "    # Extracting and returning only the text part of the response\n",
        "    return model_output['choices'][0]['text'].strip()\n",
        "Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ],
      "metadata": {
        "trusted": true,
        "id": "L6R1w_YyQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step C: Generate a response based on our dataset using RAG\n",
        "\n",
        "Refer to Step 7 if you get stuck!"
      ],
      "metadata": {
        "id": "Cxmoe9dAQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "d6zQ-uwuQ-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step D: Create a chatbot interface for the user\n",
        "\n",
        "Refer to Step 8 if you get stuck!"
      ],
      "metadata": {
        "id": "RBfJaeDVQ-8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "kkTY8Pm4Q-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of RAG + Embeddings\n",
        "* Embedding generation, semantic search and LLM inference are resource-intensive and require longer compute times\n",
        "* Requires GPU for efficieny"
      ],
      "metadata": {
        "id": "WYDXUjK-Q-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "ORM-pRzuQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Note on PEFT and Quantization in Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "sDB_zETbQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter-Efficient Fine-Tuning (PEFT)**  \n",
        "PEFT techniques allow you to fine-tune LLMs by updating only a small subset of parameters rather than the entire model. This makes training more efficient and reduces hardware requirements, ideal when working with limited resources.  \n",
        "\n",
        "**Quantization**  \n",
        "Quantization means converting model weights from a high-memory format (like 32-bit floats) to a lower one (like 8-bit integers). This helps reduce memory usage and allows large models to run on devices with less RAM and smaller GPUs. It also makes inference faster. For example, models can be run on phones or laptops instead of needing expensive servers.  \n",
        "\n",
        "**LoRA (Low-Rank Adaptation)**  \n",
        "LoRA is a technique used during fine-tuning that avoids updating all of the model's weights. Instead, it learns small changes to the model and stores them separately. These changes are computed using two smaller matrices, which means fewer parameters need to be updated. This makes training much faster and lighter.  \n",
        "\n",
        "**QLoRA**  \n",
        "QLoRA combines quantization and LoRA. It compresses model weights to 4-bit precision and then fine-tunes the model using LoRA. This lets you fine-tune large models using much less memory without sacrificing too much performance"
      ],
      "metadata": {
        "id": "9Hard2RiQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next milestone, we will look into implementing QLoRA to fine-tune Llama-2 effectively to meet the GPU constraints"
      ],
      "metadata": {
        "id": "afM6jDR9Q-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "JjwZBqyRQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 4: Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "VSQJKAVdUeVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does fine-tuning LLMs mean?**\n",
        "* Fine-tuning means adapting a pre-trained LLM to perform better on a specific task by continuing its training on a domain-specific dataset\n",
        "* The LLM learns patterns in the dataset and adjusts its internal weights slightly to adapt to that domain to get more relevant responses\n",
        "* For example, in our case, a base model like Llama-2 may just know general health facts but after fine-tuning it on our disease-symptom dataset, it will give more accurate answers\n",
        "\n",
        "In this section of the project, we will be fine-tuning LLMs for our medical chatbot"
      ],
      "metadata": {
        "id": "PAdekVRIQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over Embeddings + RAG Method\n",
        "* Better Domain Alignment: Fine-tuning tailors the model to specifc domain knowledge improving accuracy\n",
        "* Faster Inference: Without a retrieval step, fine-tuned models can respond faster"
      ],
      "metadata": {
        "id": "VDzmg5OcQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install required libraries\n",
        "\n",
        "**peft:** enables parameter-efficient training for large models  \n",
        "**datasets:** used to convert a pandas dataframe into a format that is compatible with Hugging Face's Trainer  \n",
        "**accelerate:** simeplifies mixed-precision training  \n",
        "**bitsandbytes:** enables quantization to reduce memory usage when training large models"
      ],
      "metadata": {
        "id": "764L_7oVQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Before running the cell below, please restart the session. You can do this by clicking the 3 dots on the upper right-hand corner and selecting *Restart & Clear Cell Outputs*. An error message might appear as you run the cell below, but you can carry on with the project without worrying about it!"
      ],
      "metadata": {
        "id": "-Pk7N6Y5Q-8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell only after restarting the session\n",
        "!pip install -q peft datasets accelerate bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "id": "7vQkyzdlQ-8x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Note:** Since the session has restarted, the dataset is no longer available. Please return to Milestone 1, run all the cells in that section to reload the data, and then come back to Milestone 4 once you are done!"
      ],
      "metadata": {
        "id": "KjHxNjTTQ-8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**TrainingArguments:** specifies training parameters for the LLM  \n",
        "**Trainer:** training loop abstraction to simplify model training  \n",
        "**BitsAndBytesConfig:** used for quantized training  \n",
        "**LoraConfig:** defines the configuration for LoRA fine-tuning  \n",
        "**get_peft_model:** wraps a base model with PEFT (LoRA) layers  \n",
        "**prepare_model_for_kbit_training:** prepares a model for 4-bit or 8-bit training  \n",
        "**PeftModel:** to load a LoRA-trained model for inferencing"
      ],
      "metadata": {
        "id": "202X7bz7Q-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft import PeftModel\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "_1hkQAsuQ-8y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transform the dataset into required format for fine-tuning Llama-2\n",
        "\n",
        "Llama models generally require a specific format as the input which is the `[INST] ... [/INST]` format.  \n",
        "For example, we need to transform our dataset to look like this:\n",
        "`<s>[INST] abdominal pain, fever [\\INST] Appendicitis`  \n",
        "\n",
        "1. Define a function that formats the input passed into the format we discussed above\n",
        "2. Apply the `format_prompt` function to each row of the dataframe and create a new column called **text** that stores the formatted prompt for each row\n",
        "3. Convert the dataframe into a Hugging Face `Dataset` object"
      ],
      "metadata": {
        "id": "ai_tO-AIQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "def format_prompt(row):\n",
        "    return f\"<s>[INST] {row['Symptoms']} [/INST] {row['Disease']}\"\n",
        "\n",
        "dis_symp_df[\"text\"] = dis_symp_df.apply(__________, axis=1)\n",
        "\n",
        "formatted_df = __________.from_pandas(dis_symp_df)"
      ],
      "metadata": {
        "id": "gyCd86rNUh8y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out how the first entry of `formatted_df` looks"
      ],
      "metadata": {
        "id": "2igRNRbVQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "Lss2W1h2dmud",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Load the Llama-2 chat model using QLoRA\n",
        "\n",
        "1. Specify the Hugging Face model we want to load. Here we will be using `NousResearch/Llama-2-7b-chat-hf` which is a 7B parameter version of Llama-2\n",
        "2. Set up the 4-bit quantization for QLoRA using `BitsAndBytesConfig` and define the parameter values\n",
        "3. Load the quantized model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "sSx0EnwwQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model_name = \"____________________________\"\n",
        "\n",
        "bnb_config = _________________(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = _________________.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},  # explicitly use GPU 0 (GPU T4 x2)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "U_sXc_hdQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Set up LoRA for a 4-bit quantized Llama-2 model\n",
        "\n",
        "1. Prepare the 4-bit quantized model for training using `prepare_model_for_kbit_training`\n",
        "2. Create the configuration for LoRA and define the parameter values\n",
        "3. Wrap the model with LoRA using the defined configuration. This resuts in only a small set of trainable weights which reduces compute and memory needs"
      ],
      "metadata": {
        "id": "HuYtRf1zQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model = _____________________(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # rank of the LoRA update matrices\n",
        "    lora_alpha=16, # scaling factor for the LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust based on model architecture, here we apply LoRA only to the query and value projection layers of attention\n",
        "    lora_dropout=0.1, # dropout applied to LoRA layers during training to avoid overfitting\n",
        "    bias=\"none\", # do not train the bias parameters\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUf-azeyQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Tokenize Dataset\n",
        "\n",
        "1. Load the corresponding tokenizer for our model\n",
        "2. Set the `pad_token` to be the same as the `eos_token` since models like Llama do not have separate padding token defined by default"
      ],
      "metadata": {
        "id": "AcekV-2SQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "tokenizer = _______________.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.___________"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ww85KMVdQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define function that takes one row and processes it for training\n",
        "4. Using `tokenizer` convert the input text into token IDs\n",
        "5. Set labels to be a copy of input_ids. In causal language modeling, the model is trained to predict the next token so the input and out are the same"
      ],
      "metadata": {
        "id": "WNcE7qLlQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "trusted": true,
        "id": "jwt1-Y_lQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Apply the `tokenize_function` to each row of the `formatted_df` and remove columns **text**, **Disease** and **Symptoms** to keep only the tokenized inputs"
      ],
      "metadata": {
        "id": "-Q8d04FCQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "tokenized_datasets = formatted_df.map(______________, remove_columns=[\"________\", \"________\", \"________\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ClkRdHSCQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Define training parameters for fine-tuning the Llama-2 model"
      ],
      "metadata": {
        "id": "NaUB6C6dQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"llama2-finetune\",\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    max_steps=-1,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    save_steps=0\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "nxcXrvOKQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Initialize `Trainer`\n",
        "\n",
        "Define the following parameters:  \n",
        "**model:** the LoRA-wrapped Llama-2 model we are fine-tuning  \n",
        "**args:** the training arguments we defined above  \n",
        "**train_dataset:** the tokenized dataset that contains the formatted and encoded input-output pairs  \n",
        "**tokenizer:** the tokenizer used to process inputs and decode outputs to ensure consistency between training and generation"
      ],
      "metadata": {
        "id": "0BKAyL0LQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=_____________,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=_____________,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YBLRr4V0Q-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 9: Train your LLM\n",
        "\n",
        "Finally, after the preprocessing and parameter definition, we can train our LLM!"
      ],
      "metadata": {
        "id": "LIVCm3w4Q-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qR89mmpsQ-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 10: Model Inferencing\n",
        "\n",
        "Now that we have our fine-tuned LLM, we will use it to predict possible diseases for different user inputs.\n",
        "\n",
        "1. Save the fine-tuned Llama-2 model and tokenizer to a specific directory in the Kaggle environment"
      ],
      "metadata": {
        "id": "9ADxb4OXQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/kaggle/working/llama2-med-chatbot\"\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "WOx1oDLZQ-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the base model, calling the original Llama-2 model `NousResearch/Llama-2-7b-chat-hf`\n",
        "3. Load the tokenizer from `output_dir`, set `pad_token` to `eos_token` and set `padding_side` to right which is standard for causal language models"
      ],
      "metadata": {
        "id": "Lr7JZa0gQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(_____________, trust_remote_code=True)\n",
        "tokenizer.___________ = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"_________\""
      ],
      "metadata": {
        "trusted": true,
        "id": "mH96P718Q-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load the base model with quantization using the bitsandbytes configuration define above"
      ],
      "metadata": {
        "id": "b3_LiqwbQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=______________,\n",
        "    device_map={\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8ZboWzGJQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Attach the LoRA fine-tuned weights from `output_dir` and merge them with the base model using `PeftModel`\n",
        "6. Set the model to evaluation using the `eval` function to put the model in inference mode"
      ],
      "metadata": {
        "id": "8P0sL-1TQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model = ____________.from_pretrained(base_model, output_dir)\n",
        "model._________()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8XEwOHEZQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create the chatbot interface function for the user"
      ],
      "metadata": {
        "id": "MMakv1nVQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\nNote: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        instruction = \"List the top 2 possible diseases for these symptoms:\"\n",
        "        # formatting the prompt using the required Llama-2 structure\n",
        "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{instruction}\n",
        "<</SYS>>\n",
        "\n",
        "Symptoms: {user_input} [/INST]\"\"\"\n",
        "\n",
        "        # converts prompt into token IDs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # to generate response from the model with key parameters\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                do_sample=False,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # decode the output tokens into readable text\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # extracts only the relevant part after [/INST] which contains the response\n",
        "        if \"[/INST]\" in full_response:\n",
        "            answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        print(f\"ChatBot: {answer}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "36AKil0eQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Jn19r_mnQ-83"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why do you think the fine-tuned LLM may not always give exact responses from the dataframe?\n",
        "\n",
        "**Hint:** Answer along the lines of the generative nature of LLMs, training parameters and sampling parameters (temperature, top_p)"
      ],
      "metadata": {
        "id": "-Mt51h2iQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Your answer here***"
      ],
      "metadata": {
        "id": "RhP7u23pQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your Understanding!\n",
        "\n",
        "Time to try fine-tuning an LLM by yourself!  \n",
        "BioMistral is a domain-specific version of the Mistral LLM, fine-tuned on biomedical and clinical data. It is designed to perform better on healthcare-related tasks. You can read up more about it in this [link](https://huggingface.co/BioMistral).  \n",
        "We already have our formatted dataframe, so we will start off by loading the model."
      ],
      "metadata": {
        "id": "KMqbyk3wQ-83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step A: Load `BioMistral/BioMistral-7B` using QLoRA\n",
        "\n",
        "Refer to Step 4 if you get stuck!"
      ],
      "metadata": {
        "id": "OH6tKcBiQ-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "hPhzMdXYQ-83"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step B: Set up LoRA for a 4-bit quantized BioMistral model\n",
        "\n",
        "**Hint:** For BioMistral the `target_modules` in `LoraConfig` are different since we are changing the LLM architecture. Define the `target_modules` as `[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]`.  \n",
        "Refer to Step 5 if you get stuck!"
      ],
      "metadata": {
        "id": "TwW0tBHNQ-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "joWTBCZmQ-83"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step C: Define the tokenizer for the model and tokenize the dataset\n",
        "\n",
        "Refer to Step 6 if you get stuck!"
      ],
      "metadata": {
        "id": "2tsxtgN3Q-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "UezR6lOgQ-84"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "ylQl4LOXQ-84"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step D: Define training parameters for fine-tuning\n",
        "\n",
        "**Hint:** Use the same parameters as in Step 7"
      ],
      "metadata": {
        "id": "zkQzCAPCQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "xq7dfExkQ-84"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step E: Initialize Trainer"
      ],
      "metadata": {
        "id": "4FTSBbksQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "DMmwkZjyQ-84"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step F: Train your LLM"
      ],
      "metadata": {
        "id": "SLtTDHFCQ-84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "trusted": true,
        "id": "HhT8wR6KQ-85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step G: Model Inferencing"
      ],
      "metadata": {
        "id": "eXHWGENRQ-85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "output_dir = \"/kaggle/working/biomistral-chatbot\"\n",
        "\n",
        "trainer._________________(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MhZhYHHtQ-85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "output_dir = \"/kaggle/working/biomistral-chatbot\"\n",
        "base_model_name = \"BioMistral/BioMistral-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.________________(output_dir, trust_remote_code=True)\n",
        "tokenizer.______________ = tokenizer._____________\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    _______________,\n",
        "    quantization_config=_____________,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(_____________, output_dir)\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "AQH61H8tQ-85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\nNote: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        instruction = \"List the top 2 possible diseases for these symptoms:\"\n",
        "        # formatting the prompt using the required Llama-2 structure\n",
        "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{instruction}\n",
        "<</SYS>>\n",
        "\n",
        "Symptoms: {user_input} [/INST]\"\"\"\n",
        "\n",
        "        # converts prompt into token IDs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # to generate response from the model with key parameters\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                do_sample=False,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # decode the output tokens into readable text\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # extracts only the relevant part after [/INST] which contains the response\n",
        "        if \"[/INST]\" in full_response:\n",
        "            answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        print(f\"ChatBot: {answer}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "T67XC9xRQ-85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pwC6SWEkQ-85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "Congratulations! You have successfully developed your own medical chatbots!  \n",
        "\n",
        "We would once again like to point out that these chatbots were developed solely for learning purposes and should not to be used in case of medical emergencies.\n",
        "\n",
        "To submit your amazing work please follow the steps below:\n",
        "* Rename this notebook to *[Your Name]Medical_Chatbot*\n",
        "* Download the notebook\n",
        "* Send your notebook to nsdc@nebigdatahub.org\n",
        "* Once our team receives your submission, you will be awarded with a certificate of completion!\n",
        "\n",
        "Thank you for participating in this project and please reach out to nsdc@nebigdatahub.org in case you have any questions!"
      ],
      "metadata": {
        "id": "9HlbK4iAQ-85"
      }
    }
  ]
}
